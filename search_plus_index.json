{"./":{"url":"./","title":"序言","keywords":"","body":"【操作系统最佳实践】MIT 6.828 JOS与NJU ICS NEMU源码精读 怕太长不看系列： 我做的MIT 6.828课程笔记 如果觉得有帮助的话请star一波吧~ 如果有错误欢迎指正PR，有任何问题欢迎提问。 在选择计算机专业的时候，哪一个男孩没有过做游戏的梦想？ 下面gif是NJU NEMU运行的《仙剑奇侠传》游戏。 虽然距离真正的游戏制作还非常的遥远，但令人兴奋的是，如果我们完整的学习整个系统，我们掌握到的技术远比游戏开发技术更加的广泛和深刻。 俗话说，理科生/工科生更能接近这个世界运作的真相。那么，作为一个程序员，如何去理解一个软件世界的本质呢？ 所以，我来安利大家学习操作系统了。 MIT 6.828 JOS 首先需要安利的课程就是MIT 6.828这门课程操作系统课程。 我们学习的材料是2018年的，这门课主要围绕着JOS和xv6系统进行展开，但是最主要核心系统是JOS。 MIT 6.828 2018课程主页 我做的MIT 6.828课程笔记 强烈推荐完整的学习完整门课程的内容！ JOS实验部分一个有六个大的实验，每一个实验部分都由若干个小的话题组成，大家可以选择性的查看： Lab 1: Booting a PC Lab2: Memory Management Lab3: User Environments Lab 4: Preemptive Multitasking Lab 5: File system, Spawn and Shell Lab 6: Network Driver NJU ICS NEMU 另外一个值得一读的系统就是南京大学计算机导论的课程里面涉及到的NEMU模拟器。这个模拟器是QEMU的最小系统，麻雀虽小，五脏俱全，对于我们去学习一些现代操作系统里面的内容是非常充实的。 不仅于此，NJU ICS的主讲老师蒋炎岩是位非常宝藏的老师，乐于分享自己的一些科研内容和世界观。 其中NJU ICS课程不仅有非常详尽的课程文档，该文档读起来简直就像是游戏攻略一般过瘾，还有非常宝藏老师的开源课程视频。 如果能够仔细的完成里面的PA，那么能够讲原来枯燥的操作系统知识通过代码进行学习，还能去理解一开始放上的《仙剑奇侠传》游戏运行的本质。 现代操作系统——原理与实现 最近有淘到一本好书：上交大陈海波老师的《现代操作系统——原理与实现》。 我们特别的将里面的目录拿出来，将JOS和NEMU的内容放入其中： 有一个惊人的发现，那就是由大名鼎鼎的操作系统专家陈海波写的这本书与MIT 6.828 JOS的结构惊人的相似。后来我查到，陈海波老师当年在读博期间完成了6.828的学习，并且通过JOS这样的一个系统的基础之上完成了一个新的系统，并因此收获OSDI操作系统顶会文章录用。 实际上这本书里面的chCore就是在JOS的基础之上进行的，实验内容也是惊奇的一致。 这就是我为什么安利大家可以去学习JOS和NEMU这两个系统了。倘若能够仔细的读通这两个系统的代码，在计算机软件基础知识这一块，基本上能够战胜绝大多数的程序员了吧。 最后再推荐一波资料： 我做的MIT 6.828课程笔记 世界上最牛气的操作系统教材是什么？——蒋炎岩 上海交通大学操作系统课程，与MIT6.828强相关 现代操作系统：原理与实现 "},"introduction.html":{"url":"introduction.html","title":"introduction","keywords":"","body":"6.828课程攻略 XV6由来 ​ MIT 2006年之前的计算机操作系统课程使用的是Lions' Commentary on UNIX' 6th Edition 这样的一个比较古老的教程。随着发展，这样的一个V6的UNIX系统越来越不满足学生的学习需求，因此MIT的教授们就使用ANSI C重新写了一版这个系统，并且加入更多的特性：多核、并行、锁和多线程等等，并将其命名为XV6系统。 JOS ​ 本课程中还有一个JOS的操作系统，它早在2006年的课程中存在，并且是一个辅助的材料，是同学们在内核外(exokernel)进行作业的地方。当然随着XV6的实现，JOS也发生了巨大的改变。本课程的lab1-lab6就是在JOS系统上进行的，并且当有具体的实现细节不清楚的时候，可以参照XV6的源代码。个人感觉JOS的文件管理更加的合理和科学。 资料推荐 "},"preparation.html":{"url":"preparation.html","title":"准备工作","keywords":"","body":"6.828学习准备工作 本部分是学习6.828需要做的准备工作，虽说是预备知识，但是最佳的学习效果应该是与lab1同时进行，应该该部分非常的基础且重要，于是单独成一篇文章。 环境搭建 本人使用的环境用VMware Station跑ubuntu 16.04的虚拟机。 IDE开发环境VS code和里面的插件remote ssh。 课程使用了MIT patch版本的QEMU和cgdb进行调试，cgdb安装调试可以参考MIT 6.828 2017版本环境配置 并且想要查看更具体的开发效果，可以看看我上传的视频。 汇编语言 本部分的内容主要来自该链接 并且下面所有的讲解都建立在AT&T语法的基础之上，之所以不对比，是为了防止混淆。 寄存器命名规则%eax,带有一个% 源地址、目标地址顺序：AT&T: movl src, dst 寄存赋值立即数带有$：movl $0xd00d, %ebx 寄存器后缀： b，w，l分别占1, 2, 4Bytes 寻址方式：immed32(basepointer,indexpointer,indexscale),其中basepointer是必须的。 (%eax)带有一个框号是表示eax存的值对应的地址中存放的值，间接索引。 有些指令虽然没有涉及具体的寄存器，但是会隐含的涉及一个寄存器的值和操作。 比如call address就是暗含着压栈的操作。 stos: 还会使用eax和edi寄存器，将eax的值复制到edi中。 stosl: long word stos WORD PTR ES:[EDI]: 和stos功能一致。 内联汇编 一般使用的格式为 asm (\"cld\\n\\t\" \"rep\\n\\t\" \"stosl\" : /* no output registers */ //输出部分 : \"c\" (count), \"a\" (fill_value), \"D\" (dest) //输入部分 : \"%ecx\", \"%edi\" ); //告诉寄存器，哪些寄存器被修改，不能再设用这些寄存器里面的值了 缩略字母的含义： a eax b ebx c ecx d edx S esi D edi I constant value (0 to 31) q,r dynamically allocated register (see below) g eax, ebx, ecx, edx or variable in memory A eax and edx combined into a 64-bit integer (use long longs) 关于变量的序号，是从上到下，从左到右，0，1，2...n赋予序号。 c语言指针 可以仔细看看这个链接 这一部分主要讲讲c语言里面的一些用到的语法技巧。 例子1：指针强制类型转化 int *x; *((char *) (x)+1)='a'; 就是对x指针强制类型转换，然后在该地址进行赋值。 例子2：数组初始化 pde_t entry_pgdir[NPDENTRIES] = { [0] = ((uintptr_t)entry_pgtable - KERNBASE) + PTE_P, [960] = ((uintptr_t)entry_pgtable - KERNBASE) + PTE_P + PTE_W }; //仅仅对其中的部分值进行初始化，需要有前面的[x]的索引来进行。 //后面会发现中断向量表也是这样进行的。 例子3：钩子函数 // 定义钩子函数 struct Command { const char *name; const char *desc; // return -1 to force monitor to exit int (*func)(int argc, char** argv, struct Trapframe* tf); }; //实例化 static struct Command commands[] = { { \"help\", \"Display this list of commands\", mon_help }, { \"kerninfo\", \"Display information about the kernel\", mon_kerninfo }, {\"traceback\", \"traceback info\", mon_kerninfo}, }; //具体的需要挂载到钩子函数上的实际函数 int mon_kerninfo(int argc, char **argv, struct Trapframe *tf) { extern char _start[], entry[], etext[], edata[], end[]; cprintf(\"Special kernel symbols:\\n\"); cprintf(\" _start %08x (phys)\\n\", _start); cprintf(\" entry %08x (virt) %08x (phys)\\n\", entry, entry - KERNBASE); cprintf(\" etext %08x (virt) %08x (phys)\\n\", etext, etext - KERNBASE); cprintf(\" edata %08x (virt) %08x (phys)\\n\", edata, edata - KERNBASE); cprintf(\" end %08x (virt) %08x (phys)\\n\", end, end - KERNBASE); cprintf(\"Kernel executable memory footprint: %dKB\\n\", ROUNDUP(end - entry, 1024) / 1024); return 0; } //调用 for (i = 0; i 例子4：函数可变长参数 可变参数函数实现的步骤如下: １.在函数中创建一个va_list类型变量 ２.使用va_start对其进行初始化 ３.使用va_arg访问参数值 ４.使用va_end完成清理工作 //来源：公众号【编程珠玑】 #include /*要使用变长参数的宏，需要包含下面的头文件*/ #include /* * getSum：用于计算一组整数的和 * num：整数的数量 * * */ int getSum(int num,...) { va_list ap;//定义参数列表变量 int sum = 0; int loop = 0; va_start(ap,num); /*遍历参数值*/ for(;loop 16位向32位系统转换 一般有下面的套路： flush_gdt: lgdt [gdtr] ljmp 0x08:complete_flush complete_flush: mov ax, 0x10 mov ds, ax mov es, ax mov fs, ax mov gs, ax mov ss, ax ret 具体解释为什么使用ljmp，为什么段寄存器都赋值为0x10，可以参考该链接。 GDB常用命令 si: 执行一个汇编指令 s:执行一个c语言语句，并且进入到函数中。 n:与上面的相对应，执行一个c语句，不进入到函数执行中。 b *address: 在某个地址打断点，这个地址前记得一定要带*。当然也可以是函数名，文件的行号。 p /x var:查看某个变量的值，可以是寄存器，或变量。 x/Nx address:查看从address开始的若干内存的内容。 x/s address:查看字符串的内容。 bt: 查看栈的内容，如lab 1 exercise 12的实现功能一致。 建议有不会的内容可以查看该手册 ELF解析 建议直接参考这张图,由于这张图上有丰富的信息，因此页面展示非常模糊，直接原图下载查看。 "},"lab1-Booting a PC.html":{"url":"lab1-Booting a PC.html","title":"Lab1: Booting a PC","keywords":"","body":"Lab 1: Booting a PC 本科的时候老师拉跨的讲课水平，使得我对操作系统的理解依旧只会开关机。并且，我对操作系统的知识非常的感兴趣，因此想要深入的进行学习。网上搜索，发现非常多的该课程的攻略。如何使得自己的攻略与众不同呢？同时如何让自己更好的掌握某项知识，最好的方法就是讲给别人听。于是，在一个月之前，我在一个夜黑风高的晚上，录制了讲解的视频，链接如下。 MIT 6.828 lab1 由于录制的屏幕适配性，所以缺少了右边和下边的部分屏幕内容，但是并不影响观看。 录完该视频后，我反复看了这个视频，发现有很大的问题——虽然自己能够理解，但是讲的不清楚。讲的不成体系，所有的知识点非常的零散，并且是参考官方的tutorial讲解的，其中遗漏了很多的细节。我个人觉得我需要一个同好与我battle，这样可能能够更好的让我知道怎么讲更合适。 于是，我暂缓了录视频，开始做其他的lab。 最近，我开始重拾了该教程，目的就是做到与众不同。但是，我发现真的很难。与此同时，我尽可能多的去收集了一些操作系统的一些更深层次的设计原理，使我对其有更深的理解。如BIOS为什么从0x7c00进行启动，其背后居然有一段历史原因【4】。加入了更多的基础的讲解，如汇编语言、内联汇编、图解ELF，stack等等。lab1讲解攻略配有18张图，总字数约为一万字。 写这个攻略前我已经充分熟悉lab1了，但是依旧花了我四天内所有的空闲时间。 课程代码GitHub链接： github 代码链接 该文档的集合链接： 文档github链接 如果你觉得这个攻略对你有帮助，欢迎在githhub上star设置follow。 当然，如果你发现本文本有逻辑上的错误，欢迎提issue or pull request。 思维导图 Part 1: PC Bootstrap exercise 1 熟悉汇编语言 熟悉汇编语言，其中可以特别需要阅读材料Brennan's Guide to Inline Assembly。这个链接中很好的说明了本实验使用的汇编语言版本AT&T与Intel版本的区别，并且有有关内联汇编比较简洁够用的语法介绍。 Simulating the x86 操作系统是真实的运行在硬件之上的。本课程为了调试的简便性，使用了QEMU，该工具能够虚拟各种硬件设备，供本实验操作系统JOS进行调用，并且JOS是运行在QEMU之上。 cd lab make qemu # 编译运行JOS make qemu-nox # 不使用虚拟化的VGA窗口 make qemu-gdb # 开启gdb调试，这个时候需要在另外一个命令行使用make gdb make qemu-nox-gdb # 如果想要推出qemu，使用Ctrl+a -> x。 另外，如果想要将该系统运行到真是的硬件设备上，那么需要将obj/kern/kernel.img烧录到硬盘扇区最开始的地方。 The PC's Physical Address Space +------------------+ 这是一个主机的物理内存的分布情况。 一开始的8088处理器只能寻16位地址，能够管理1MB的物理内存。之所以16位地址能够管理1MB的物理内存地址是因为采用(CS:IP)的寻址方式，physical address = 16*segment + offset。 你问为什么不采用physical address = (segment，这样有4GB的内存可以寻址啊，并且是可以实现的，个人猜想可能和当时的硬件发展有关。 BIOS(basic input output system)的初始化程序一开始在不可擦除的flash中保存，随着内存的发展目前存放在非易失可擦写的内存中。 一般超过32位地址都是用于PCI设备寻址使用。 又由于现在有些设备支持超过4GB的内存，因此，BIOS需要为哪些PCI设备预留一些空间，提供访问，即哪怕你的物理内存超过了4GB，有些部分依旧是不能访问的，需要预留给物理设备。 The ROM BIOS Type \"apropos word\" to search for commands related to \"word\". + target remote localhost:26000 warning: A handler for the OS ABI \"GNU/Linux\" is not built into this configurati on of GDB. Attempting to continue with the default i8086 settings. The target architecture is assumed to be i8086 [f000:fff0] 0xffff0: ljmp $0xf000,$0xe05b 0x0000fff0 in ?? () + symbol-file obj/kern/kernel IBM PC BIOS第一条指令执行地址在0xffff0。可以看到在BIOS分配的区域非常上面，因此后面第一条指令就是跳到相对低的地址进行指令的执行。 exercise 2 查看BIOS执行的指令 看看BIOS开始的阶段都干啥了？ 具体指令的汇总： 1 0xffff0: ljmp $0xf000, $0xe05b #跳转到0xfe05b 2 0xfe05b: cmpl $0x0, $cs:0x6ac8 #根据cs:0x6ac8是否为0，进行跳转 3 0xfe062: jne 0xfd2e1 #无跳转 4 0xfe066: xor %dx, %dx #%dx清零 5 0xfe068: mov %dx, %ss #%ss置零 6 0xfe06a: mov $0x7000, %esp 7 0xfe070: mov $0xf34d2,%edx 8 0xfe076: jmp 0xfd15c 9 0xfd15c: mov %eax, %ecx 10 0xfd15f: cli #BIOS的启动不允许中断，关闭中断 11 0xfd160: cld #设置指令增长的方向 12 0xfd161: mov $0x8f, %eax 13 0xfd167: out %al, $0x70 #0x70和0x71是用于操作CMOS 的端口 14 0xfd169: in $0x71, %al #12-14三条指令是用于关闭不可屏蔽中断 15 0xfd16b: in $0x92, %al 16 0xfd16d: or $0x2, %al 17 0xfd16f: out %al, $0x92 18 0xfd171: lidtw %cs:0x6ab8 #将0xf6ab8处的数据读入到中断向量表寄存(IDTR) 19 0xfd177: lgdtw %cs:0x6a74 #并将0xf6a74的数据读入到全局描述符表格寄存器(GDTR)中 20 0xfd17d: mov %cr0, %eax 21 0xfd180: or $0x1, %eax 22 0xfd184: mov %eax, %cr0 #CR0末位置1，进入保护模式 23 0xfd187: ljmpl $0x8, $0xfd18f 24 0xfd18f: mov $0x10, %eax 25 0xfd194: mov %eax, %ds 26 0xfd196: mov %eax, %es 27 0xfd198: mov %eax, %ss 28 0xfd19a: mov %eax, %fs 29 0xfd19c: mov %eax, %gs 30 0xfd19e: mov %ecx, %eax #第23~29 步用于重新加载段寄存器，在加载完GDTR 寄存器后需要 #刷新所有的段寄存器的值 其中有一段需要特殊解释，就是加载GDT(Global Descriptor Table )的一个操作，具体的解释可以参考【1】： 23 0xfd187: ljmpl $0x8, $0xfd18f 24 0xfd18f: mov $0x10, %eax 25 0xfd194: mov %eax, %ds 26 0xfd196: mov %eax, %es 27 0xfd198: mov %eax, %ss 28 0xfd19a: mov %eax, %fs 29 0xfd19c: mov %eax, %gs 30 0xfd19e: mov %ecx, %eax GDT长度为48B，结构如下： |LIMIT|----BASE----| 低 高 高32位表示这个表所在的位置，低16位表示表的大小。 ljmp能够跳转的范围是(1jmp能够跳转的范围为-127+128，占用指令大小为1B。 并且上面的ljmp跳转用法为： Long jump, use 0xfebc for the CS register and 0x12345678 for the EIP register: # 并且CS寄存器的值不能使用mov进行修改，但是下面的指令可以修改CS寄存器的值为0xfebc ljmp $0xfebc, $0x12345678 为什么要将所有与段寄存有关的寄存器都置为0x10。 这是段寄存器结构: 0x10 = 00010000 = 000010 0 00 也就是说优先级是最高的，选择的表是GDT，segment index = 2。 Q：为什么加载GDT需要刷新段寄存器呢？ 因为GDT初始化了表的位置，剩下的索引需要依靠段寄存器，进行偏移索引。 最后总结BIOS做的事情： 初始化IDT和GDT 初始化VGA设备，PCI bus挂载的各类设备 找到物理存储设备，并且找到一个bootable disk，将运行的权利转移到disk。 Part 2: The Boot Loader 磁盘被分割为sector的最小基本单位，1 sector=512B。 后来随着硬件的发展，PC主要也需要能够从CD中启动，而CD中分割的基本单位为1 sector = 2048B。方式略有不同，具体不同需要参考其他的资料。 boot loader主要做的两件事情： 从16-bit的实模式转化到32-bit的保护模式。转换到32位的保护模式有诸多的好处，我们在之后转换额过程中进行具体的讨论。 第二点是将kernel代码从硬盘中读取出来。这个过程涉及到特殊的I/O指令。对理解操作系统整体的框架来说，这并不是一个重要的点。 exercise 3 熟悉GDB的调试指令。 使用GDB单步进行调试，调试的时候可以参考obj/boot/boot.asm这个反汇编中的指令，因为CPU就是按照这个文件里面的指令进行的。 readsect()里面调用的指令和读取完了执行了什么指令。 这一部分主要是熟悉使用GDB结合.asm文件进行调试。实际上我们使用的调试环境是cgdb，因此能够实时的看到反汇编的代码。当然调试的过程中需要对整体有一个理解的过程，因此这个时候需要借鉴.asm。 Q1： 什么时候处理器开始执行32位的代码？什么引起了从16-转换到了32-bit？ ljmp $PROT_MODE_CSEG, $protcseg 可以看到，在执行这个指令之前，地址的寻址方式为[0:7c2d]，一旦执行完这条语句，寻址方式变为0x7c32。 Q2: bootloader执行的最后一个语句？kernel执行的第一条语句为？ movw $0x1234,0x472 # warm boot Q3: kernel执行的第一条指令？ 同Q2的第二个答案。 movw $0x1234,0x472 # warm boot Q4: boot loader是怎么知道读取多少个sectors到内存的？ 这些信息保存到ELF header中，在编译的时候就已经决定了。 关于程序是怎么加载到内存中并且进行运行的，主要看两张图进行理解： 可以看到可以从ELF header读取到Program header table和Section header table的信息，之后能够从这两个表中读取更详细的信息。 通过下面的图，我们可以知道ELF运作更多的细节： 这张图上传后压缩肯定看不清了，建议直接下载看原图:原图链接 在ELF header中记录着program header table的个数，每个entry的大小，偏移量。而program header table中每一个具体的信息都能够帮助我们将具体的指令，数据读入到内存中。 section header table在读取数据进内存没有什么用。 真正起到将数据读到内存的流程就是ELF header -> 查找 program header table -> 具体的数据。 Loading the Kernel exercise 4 首先熟悉C语言的指针。一个比较简短的教材。 解释下面的程序输出内容。 #include #include void f(void) { int a[4]; int *b = malloc(16); int *c; int i; printf(\"1: a = %p, b = %p, c = %p\\n\", a, b, c); c = a; for (i = 0; i pinter.c的输出内容为： 1: a = 0x7ffec83ade20, b = 0x13e3010, c = 0xf0b6ff 2: a[0] = 200, a[1] = 101, a[2] = 102, a[3] = 103 3: a[0] = 200, a[1] = 300, a[2] = 301, a[3] = 302 4: a[0] = 200, a[1] = 400, a[2] = 301, a[3] = 302 5: a[0] = 200, a[1] = 128144, a[2] = 256, a[3] = 302 6: a = 0x7ffec83ade20, b = 0x7ffec83ade24, c = 0x7ffec83ade21 其他的输出原因比较的简单，我们仅仅分析第5条的输出结果： 修改前a内存的分布 0000 0000 高地址 0000 0000 0000 0001 0010 1101 0000 0000 0000 0000 0000 0001 1001 0000 低地址 a+1 修改后a内存的分布 0000 0000 高地址 0000 0000 0000 0001 --------- 0000 0000 0000 0000 0000 0001 1111 0100 --------- 1001 0000 低地址 a+1 这个例子也证明该PC的内存使用的是小端法，即数字的低位保存在低地址。 该实验主要想要证明的是，不同类型的偏移量是不同的，和变量的类型有关。 下面一部分是熟悉ELF(Executable and Linkable Format)。 二进制文件有一个固定长度的ELF header，紧随其后的是可变长的program header，其中包含着需要被读取到内存中的program section。 其中需要特别留意几个常用的section： .text: 这个程序中可执行的指令。 .rodata: 只读的数据，一般是C语言的常量，比如printf中保存的值。 .data: 程序中初始化了的全局变量。 .bss: 程序中未初始化的全局变量，紧跟在.data后面。由于这些值在程序运行的时候都是0，因此在ELF记录的时候，仅需要记录这一段的起始地址和大小。等加载(program loader)时，再将对应的空间进行初始化。 输入： tbl@ubuntu:~/6.828/lab$ objdump -h obj/kern/kernel 输出 tbl@ubuntu:~/6.828/lab$ objdump -h obj/kern/kernel obj/kern/kernel: file format elf32-i386 Sections: Idx Name Size VMA LMA File off Algn 0 .text 00001941 f0100000 00100000 00001000 2**4 CONTENTS, ALLOC, LOAD, READONLY, CODE 1 .rodata 0000079c f0101960 00101960 00002960 2**5 CONTENTS, ALLOC, LOAD, READONLY, DATA 2 .stab 00003b89 f01020fc 001020fc 000030fc 2**2 CONTENTS, ALLOC, LOAD, READONLY, DATA 3 .stabstr 0000197f f0105c85 00105c85 00006c85 2**0 CONTENTS, ALLOC, LOAD, READONLY, DATA 4 .data 0000a300 f0108000 00108000 00009000 2**12 CONTENTS, ALLOC, LOAD, DATA 5 .bss 00000648 f0112300 00112300 00013300 2**5 CONTENTS, ALLOC, LOAD, DATA 6 .comment 00000035 00000000 00000000 00013948 2**0 CONTENTS, READONLY 需要特别注意VMA=link address=vaddr(虚拟地址)， LMA=load address=paddr(物理地址)。 The load address of a section is the memory address at which that section should be loaded into memory. The link address of a section is the memory address from which the section expects to execute. 一般link和load地址都是相同的，如下例： tbl@ubuntu:~/6.828/lab$ objdump -h obj/boot/boot.out obj/boot/boot.out: file format elf32-i386 Sections: Idx Name Size VMA LMA File off Algn 0 .text 00000186 00007c00 00007c00 00000074 2**2 CONTENTS, ALLOC, LOAD, CODE 1 .eh_frame 000000a8 00007d88 00007d88 000001fc 2**2 CONTENTS, ALLOC, LOAD, READONLY, DATA 2 .stab 00000720 00000000 00000000 000002a4 2**2 CONTENTS, READONLY, DEBUGGING 3 .stabstr 0000088f 00000000 00000000 000009c4 2**0 CONTENTS, READONLY, DEBUGGING 4 .comment 00000035 00000000 00000000 00001253 2**0 CONTENTS, READONLY 我们对下面的 tbl@ubuntu:~/6.828/lab$ objdump -x obj/kern/kernel obj/kern/kernel: file format elf32-i386 obj/kern/kernel architecture: i386, flags 0x00000112: EXEC_P, HAS_SYMS, D_PAGED start address 0x0010000c Program Header: LOAD off 0x00001000 vaddr 0xf0100000 paddr 0x00100000 align 2**12 filesz 0x00007604 memsz 0x00007604 flags r-x LOAD off 0x00009000 vaddr 0xf0108000 paddr 0x00108000 align 2**12 filesz 0x0000a948 memsz 0x0000a948 flags rw- STACK off 0x00000000 vaddr 0x00000000 paddr 0x00000000 align 2**4 filesz 0x00000000 memsz 0x00000000 flags rwx Sections: Idx Name Size VMA LMA File off Algn 0 .text 00001941 f0100000 00100000 00001000 2**4 CONTENTS, ALLOC, LOAD, READONLY, CODE 1 .rodata 0000079c f0101960 00101960 00002960 2**5 CONTENTS, ALLOC, LOAD, READONLY, DATA 2 .stab 00003b89 f01020fc 001020fc 000030fc 2**2 CONTENTS, ALLOC, LOAD, READONLY, DATA 3 .stabstr 0000197f f0105c85 00105c85 00006c85 2**0 CONTENTS, ALLOC, LOAD, READONLY, DATA 4 .data 0000a300 f0108000 00108000 00009000 2**12 CONTENTS, ALLOC, LOAD, DATA 5 .bss 00000648 f0112300 00112300 00013300 2**5 CONTENTS, ALLOC, LOAD, DATA 6 .comment 00000035 00000000 00000000 00013948 2**0 CONTENTS, READONLY SYMBOL TABLE: f0100000 l d .text 00000000 .text f0101960 l d .rodata 00000000 .rodata Program Header中LOAD表示会加载到内存中，可以从filesz和vaddr推断：.text,.rodata...stabstr这些部分在第一个LOAD部分，后面的所有sections都在第二个LOAD中。 exercise 5 修改boot loader的地址，然后看看会发生什么错误。 BIOS从0x7c00这个地址开始执行boot loader中的程序，因此这个地址是这个sector的load address。并且这个地址可以在boot/Makefrag进行修改。 将原来的值修改为0x8c00，我们重新在0x7c00这个地址打上断点，发现还是在能够进入boot loader的，并且似乎没有什么变化。我们si单步执行下去，发现在这样的语句出现了错误： ljmp $0x8, $0x8c32 第二个值原来是0x7c32可以看到是这里产生了改变。 并且为什么我们修改了这个加载的地址，BIOS启动执行的第一条指令的地址依旧是0x7c00呢？这个其实有一个典故，和早期的计算机的设计是有关系的【3】。历史上有一种机器的内存为32KB，0x7C00-0x7DFF中前512B存放硬盘的引导程序，后512B存放其中产生的数据。 我们可以看到boot loader的load address和link address是一致的，但是在kernel部分并不是一致的。也就是说，kernel希望boot loader能够在低地址加载自己，在高地址(0xf0000...)执行自己。这个怎么实现的，后面会详细的讨论。 查看程序执行的开始地址： tbl@ubuntu:~/6.828/lab$ objdump -f obj/kern/kernel obj/kern/kernel: file format elf32-i386 architecture: i386, flags 0x00000112: EXEC_P, HAS_SYMS, D_PAGED start address 0x0010000c exercise 6 使用x/Nx address查看内存中的内容 当从BIOS进入到boot loader时在地址0x00100000的内存存储的内容。 进入到kernel 0x00100000存的值。 一般情况下1 word = 2 Bytes 两处内存的值分别为： 我们再查看obj/kern/kernel.asm中指令： entry: movw $0x1234,0x472 # warm boot f0100000: 02 b0 ad 1b 00 00 add 0x1bad(%eax),%dh f0100006: 00 00 add %al,(%eax) f0100008: fe 4f 52 decb 0x52(%edi) f010000b: e4 .byte 0xe4 f010000c : f010000c: 66 c7 05 72 04 00 00 movw $0x1234,0x472 f0100013: 34 12 从下面这个片段，我们知道0x34存在地址0xf0100013, 0x12存到了0xf0100014，但是在GDB展示的时候是逆过来的，说明内部是倒序的。 f010000c: 66 c7 05 72 04 00 00 movw $0x1234,0x472 f0100013: 34 12 我们发现，GDB一个32位的值的展示，地址是从高->低的。 可以发现和上面图片所展示的内容一致。 Part 3: The Kernel 从这一部分开始，我们就要开始写一些代码了！ Using virtual memory to work around position dependence 我们在做之前的练习5的时候发现，kernel的link address和load address是不相同的，这一部分我们就详细的进行探索。 设置kernel的link address在/kern/kernle.ld中进行设置，可以明显的看到这样的一个语句： /* Link the kernel at this address: \".\" means the current address */ . = 0xF0100000; 系统的内核一般是将内核程序放到非常高的虚拟空间进行执行，低一些的空间留给用户执行程序。尽管内核运行在较高的虚拟地址中，但是实际保存的物理地址依旧是1MB之上，也就是在BIOS之上。 在本实验中，我们仅仅会使用到0xf0000000-0xf0400000这一段的虚拟地址，我们将其映射到0x00000000-0x00400000 。并且我们也将0x00000000-0x00400000映射到同样一块物理地址。目前仅使用了4MB大小的内存。 关于目前的这个系统是怎么进行内存的映射的，主要使用了cr3和cr0。 并且寻址方式如下： 不同的是，目前lab1仅仅初始化了page directory中的两条数据，和page table中所有的数据，如下面的红框标注所示： cr3寄存器确定了这张表的基地址 # Load the physical address of entry_pgdir into cr3. entry_pgdir # is defined in entrypgdir.c. movl $(RELOC(entry_pgdir)), %eax movl %eax, %cr3 exercise 7 在movl %eax, %cr0语句停止执行，观察前后地址0x00100000和0xf0100000内容的变化。 将上面这句注释了，查看会发生什么？第一条崩溃的指令 我们在0x10000c打上断点，并且在该语句前后查看此时两个地址中的内容： 此时两个地址内容是不同的。 一旦执行了该语句： 两个地址指向了同样的内容。 后来我们将这个语句进行注释，重新执行，发现在这个语句发生崩溃： mov $relocated, %eax f0100028: b8 2f 00 10 f0 mov $0xf010002f,%eax jmp *%eax f010002d: ff e0 jmp *%eax 原因也很简单，0xf010002f这个地址并没有被映射，因此无法访问，产生错误。 Formatted Printing to the Console 这一部分就是教我们printf的底层原理是什么，相当于手写printf。 exercise 8 找到cprintf(\"6828 decimal is %o octal!\\n\", 6828);使之能够输出进制为8。 找到相应的需要实现的部分在printfmt.c,代码如下： // (unsigned) octal case 'o': // Replace this with your code. num = getuint(&ap, lflag); base = 8; goto number; break; 首先我们了解一下c语言中的变长参数机制。 首先验证函数调用的压栈机制，运行如下的程序： // gcc -m32 -o test ./test.c // 注意是32位的编译器进行编译 #include void test(int a,int b,int c,int d,int e,int f,int g,int h) { printf(\"%p\\n%p\\n%p\\n%p\\n%p\\n%p\\n%p\\n%p\\n\",&a,&b,&c,&d,&e,&f,&g,&h); } int main(int argc,char *argv[]) { int a = 1; int b = 2; int c = 3; int d = 4; int e = 5; int f = 6; int g = 7; int h = 8; test(a,b,c,d,e,f,g,h); return 0; } 输出结果为： 0xff842030 0xff842034 0xff842038 0xff84203c 0xff842040 0xff842044 0xff842048 0xff84204c 我们知道栈空间应该是从高地址向低地址的，因此压栈的顺序是从右往左。 编译器有三个宏定义： va_list ：存储参数的类型信息，32位和64位实现不一样。 void va_start ( va_list ap, paramN ); 参数： ap: 可变参数列表地址 paramN: 确定的参数 功能：初始化可变参数列表，会把paraN之后的参数放入ap中 type va_arg ( va_list ap, type ); 功能：返回下一个参数的值。 void va_end ( va_list ap ); 功能：完成清理工作。 可变参数函数实现的步骤如下: １.在函数中创建一个va_list类型变量 ２.使用va_start对其进行初始化 ３.使用va_arg访问参数值 ４.使用va_end完成清理工作 结合具体的例子来看： //来源：公众号【编程珠玑】 #include /*要使用变长参数的宏，需要包含下面的头文件*/ #include /* * getSum：用于计算一组整数的和 * num：整数的数量 * * */ int getSum(int num,...) { va_list ap;//定义参数列表变量 int sum = 0; int loop = 0; va_start(ap,num); /*遍历参数值*/ for(;loop 可以看到，当调用变长函数的时候，左边的参数必须是固定的，并且变长参数列表应该和固定参数中一一对应。 这下其实我们就能够理解其中的原理了。 接下来进行下面的实验，查看各类打印函数的调用关系。 kern/printf.c, lib/printfmt.c, and kern/console.c 从左往右，越来越底层。其中printf.c定义了一个比较简单的供用户使用的抽象函数，我们知道变长参数函数调用的原理后，就能够明白printfmt.c是在解析固定的那个字符串，按照规定的格式读出变长参数。最后通过最为底层的console.c与底层的硬件进行交互。 Q1：printf.c是怎么去调用console.c中的函数的？ printf.c 调用console.c中的cputchar 函数进行显示输出。 Q2：解释下面的代码是干什么的？ 1 if (crt_pos >= CRT_SIZE) { 2 int i; 3 memmove(crt_buf, crt_buf + CRT_COLS, (CRT_SIZE - CRT_COLS) * sizeof(uint16_t)); //memmove(void *dst, const void *src, size_t len) 4 for (i = CRT_SIZE - CRT_COLS; i 当显示器显示的字符数量超过限制的时候，首先是将crt_buf + CRT_COLS开始到结束的字符移到最开始的地方，这个时候末尾的 CRT_COLS个显示位置空出来了，并且这个时候将其置为空格。 Q3: 单步执行下面的语句 int x = 1, y = 3, z = 4; cprintf(\"x %d, y %x, z %d\\n\", x, y, z); 我们将其插入到init.c中。 使用下面的常用的几条命令： n: 执行单条c语言语句，不进入到具体的函数中。 s(tep): 进入到具体的函数中。 bt(backtrace): 打印出函数调用栈。 得到如下的结果： 使用x/s fmt打印出了常量字符串 x/3wx打印出来了可变参数内容 课程教程中让我们在cons_putc, va_arg和vcprintf每次调用的参数变化。 其中va_arg由于是一个宏定义，我们换成其他的函数getuint打断点，并且能够观察ap(可变参数)的变化。 cons_putc函数是每次输出一个字符。 我们分别在三个函数上打上断点，观察它们的变化： 函数 值 说明 1 vcprintf fmt=“x %d, y %x, z %d \\n”, ap=1,3,4 2 cons_putc c=120 'x' 3 cons_putc c=32 ' ' 4 va_arg 调用前ap=1,3,4 调用后ap=3,4 5 cons_putc 49 '1' 6 cons_putc 44 ',' 7 cons_putc 32 ' ' 8 cons_putc 121 'y' 9 cons_putc 32 ' ' 10 va_arg 调用前ap=3,4 调用后ap=4 11 cons_putc 51 '3' 12 cons_putc 44 ',' 13 cons_putc 32 ' ' 14 cons_putc 122 'z' 15 cons_putc 32 ' ' 16 va_arg 调用前ap=4 调用后ap 为空 17 cons_putc 52 '4' 18 cons_putc 10 '\\n' Q4: 运行下面的代码，并且解释为什么是这样的输出结果。 unsigned int i = 0x00646c72; cprintf(\"H%x Wo%s\", 57616, &i); 输出He110 World $(57616){10} = (e110){16}$ 由于是小端存储，在一个字节内，最开始的地方是地位，并且我们找到反汇编语言的地方： //cprintf(\"x %d, y %x, z %d\\n\", x, y, z); unsigned int i = 0x00646c72; //地址 c8 c9 ca cb cc cd ce f01000c8: c7 45 f4 72 6c 64 00 movl $0x646c72,-0xc(%ebp) cprintf(\"H%x Wo%s\", 57616, &i); f01000cf: 83 c4 0c add $0xc,%esp 我们按照地址增的顺序打印了内容，进一步验证了小端法的保存方式。 并且ASCII码0x72='r', 0x6c='l',0x64='d',0x00='\\0'(null) Q5: 运行cprintf(\"x=%d y=%d\", 3);并且解释输出的结果。 我们首先查看这个部分的反汇编部分 cprintf(\"x=%d y=%d\", 3); f01000c8: 83 c4 08 add $0x8,%esp f01000cb: 6a 03 push $0x3 f01000cd: 68 b2 19 10 f0 push $0xf01019b2 f01000d2: e8 d5 08 00 00 call f01009ac 然后观察栈空间的变化： 发现栈中依次压入了0x3, 0xf01019b2确实在内存中显示了。 最后输出的结果为1600。在栈空间中，0x3对应的后面一个空间存放的是0x00000640 = 1600。也就是说，虽然，我们没有指定需要输出的变量，但是程序还是会从该变量的后面读取相应的内存并且输出。 Q6: 如何函数压栈的顺序为从左到右，那么cprintf应该怎么修改，使得其能够正确的输出？ 这个答案我不确定，感觉是va_arg首先通过fmt确定整个输出的大小，然后找到头，移动方向与原来相反。 有的答案说的是int cprintf(..., const char *fmt)感觉不对。（欢迎大佬来讨论） The Stack exercise 9 找到kernel初始化栈的地方。 kernel是如何为栈保留空间的 这段初始程序最后栈的空间分布 在entry.S中有这样一段代码进行初始化： #part 1 movl $0x0,%ebp # nuke frame pointer # exercise 11提示看这个文件内容，发现应该是这个地方暗示着栈的结束。 # Set the stack pointer movl $(bootstacktop),%esp # part 2 bootstack: .space KSTKSIZE .globl bootstacktop # part 3 #define KSTKSIZE (8*PGSIZE) 在实际运行的时候，进行了这样的初始化： 9│ 0x10002f: mov $0x0,%ebp 10│ 0x100034: mov $0xf0110000,%esp 1 PGSIZE=4096，通过计算，虚拟地址为0xf010f000~0xf0117000, 对用物理地址0xf010f000~0x00117000，并且栈空间大小为32KB。 因为栈是从高地址向低地址变化的，esp是栈的最低的地址。 ebp是栈底，高地址位。 一般产生函数调用时，会有以下的模式套路： push $ebp mov $esp $ebp 有了这样的模式之后，我们就能链式的进行溯源，查看调用的关系。 最后我们进行函数调用的时候，一般步骤，一定要注意顺序！ 从右往左压入变量参数 压入函数返回地址，也就是调用的地方地址后面。 压入ebp，将esp赋值给ebp 保存寄存器值，局部变量 最终得到这样的结构图，红框表示一次完整的函数调用： 这样的一个栈的空间分布其实是非常有用的，这样当某个函数运行崩溃之后，我们可以不断的回溯ebp的值，并且读取栈空间中的值，就能够找到函数调用关系和，最终能够找到问题的源头。 exercise 10 在test_backtrace函数打上断点，观察每次递归时函数参数在栈中的变化。 同时，我们需要将该功能hook到monitor中进行调用。 什么是钩子函数，就是定义一个普遍的接口，当需要调用不同的函数功能的时候，往上添加就可以了，一般的定义如下： // 定义钩子函数 struct Command { const char *name; const char *desc; // return -1 to force monitor to exit int (*func)(int argc, char** argv, struct Trapframe* tf); }; //实例化 static struct Command commands[] = { { \"help\", \"Display this list of commands\", mon_help }, { \"kerninfo\", \"Display information about the kernel\", mon_kerninfo }, {\"traceback\", \"traceback info\", mon_kerninfo}, }; //具体的需要挂载到钩子函数上的实际函数 int mon_kerninfo(int argc, char **argv, struct Trapframe *tf) { extern char _start[], entry[], etext[], edata[], end[]; cprintf(\"Special kernel symbols:\\n\"); cprintf(\" _start %08x (phys)\\n\", _start); cprintf(\" entry %08x (virt) %08x (phys)\\n\", entry, entry - KERNBASE); cprintf(\" etext %08x (virt) %08x (phys)\\n\", etext, etext - KERNBASE); cprintf(\" edata %08x (virt) %08x (phys)\\n\", edata, edata - KERNBASE); cprintf(\" end %08x (virt) %08x (phys)\\n\", end, end - KERNBASE); cprintf(\"Kernel executable memory footprint: %dKB\\n\", ROUNDUP(end - entry, 1024) / 1024); return 0; } //调用 for (i = 0; i exercise 11 完成mon_backtrace()函数，使得该函数能够展示调用时刻的ebp, 函数返回时执行的指令地址eip(实际上eip指的是当前执行指令的地址，这里的含义不一样),和5个变量arg。 就是递归的查看函数调用的关系，输出栈空间的变化： Stack backtrace: ebp f0109e58 eip f0100a62 args 00000001 f0109e80 f0109e98 f0100ed2 00000031 ebp f0109ed8 eip f01000d6 args 00000000 00000000 f0100058 f0109f28 00000061 ... 需要注意的点就是，我们前面知道ebp初始化为0，因此终止的条件就是0。 通过观察栈空间分布，返回地址就是ebp+1。 由于这个函数调用参数是相对固定的，因此我们输出5个参数。 实现的代码： cprintf(\"Stack backtrace:\\n\"); uint32_t ebp, eip; for(ebp=read_ebp(); ebp!=0; ebp=*((uint32_t *)ebp)){ eip = *((uint32_t *)ebp+1); cprintf(\" ebp %08x eip %08x args %08x %08x %08x %08x %08x\\n\", ebp, eip, *((uint32_t *)ebp+2), *((uint32_t *)ebp+3), *((uint32_t *)ebp+4), *((uint32_t *)ebp+5), *((uint32_t *)ebp+5), *((uint32_t *)ebp+6)); } exercise 12 完善debuginfo_eip(eip),使得调用该函数，我们能够得到下面的信息： 文件名称 代码行号 eip在函数内字节数的偏移 并且按照下面的格式进行输出：(有用的函数printf(\"%.*s\", length, string)，该输出表示最多展示length的字符个数，若小于该值则全部输出) ebp f010ff78 eip f01008ae args 00000001 f010ff8c 00000000 f0110580 00000000 kern/monitor.c:143: monitor+106 首先看看我们能够拿到什么信息用来分析，使用命令objdump -G obj/kern/kernel，得到如下的部分输出： tbl@ubuntu:~/6.828/lab$ objdump -G obj/kern/kernel | head -n 100 obj/kern/kernel: file format elf32-i386 Contents of .stab section: Symnum n_type n_othr n_desc n_value n_strx String 1 2 3 4 5 6 7 //改行是我自己加的 -1 HdrSym 0 1269 0000197e 1 0 SO 0 0 f0100000 1 {standard input} 1 SOL 0 0 f010000c 18 kern/entry.S 2 SLINE 0 44 f010000c 0 3 SLINE 0 57 f0100015 0 4 SLINE 0 58 f010001a 0 5 SLINE 0 60 f010001d 0 6 SLINE 0 61 f0100020 0 7 SLINE 0 62 f0100025 0 8 SLINE 0 67 f0100028 0 9 SLINE 0 68 f010002d 0 10 SLINE 0 74 f010002f 0 11 SLINE 0 78 f0100034 0 12 SLINE 0 81 f0100039 0 13 SLINE 0 84 f010003e 0 14 SO 0 2 f0100040 31 kern/entrypgdir.c 15 OPT 0 0 00000000 49 gcc2_compiled. 16 LSYM 0 0 00000000 64 int:t(0,1)=r(0,1);-2147483648;2147483647; 17 LSYM 0 0 00000000 106 char:t(0,2)=r(0,2);0;127; 参考Stab的结构体，对应2-6列： struct Stab { uint32_t n_strx; // index into string table of name uint8_t n_type; // type of symbol uint8_t n_other; // misc info (usually empty) uint16_t n_desc; // description field uintptr_t n_value; // value of symbol，就是执行的地址 }; 第七列是stabstr。 上面的内容还是很杂乱，我们看到二分搜索的主要类型有N_SO(源文件名称)，N_FUN(函数名称)等等。 我们将上面的内容输出来看看： objdump -G obj/kern/kernel | grep \"SOL\" 1 SOL 0 0 f010000c 18 kern/entry.S 176 SOL 0 0 f0100189 2985 ./inc/x86.h 178 SOL 0 0 f010018f 2970 kern/console.c 180 SOL 0 0 f0100193 2985 ./inc/x86.h 182 SOL 0 0 f0100199 2970 kern/console.c 201 SOL 0 0 f01001e8 2985 ./inc/x86.h 203 SOL 0 0 f01001ee 2970 kern/console.c 206 SOL 0 0 f01001fe 2985 ./inc/x86.h 208 SOL 0 0 f0100206 2970 kern/console.c 230 SOL 0 0 f01002dc 2985 ./inc/x86.h 232 SOL 0 0 f01002ea 2970 kern/console.c 248 SOL 0 0 f0100311 2985 ./inc/x86.h 250 SOL 0 0 f0100323 2970 kern/console.c 252 SOL 0 0 f0100326 2985 ./inc/x86.h 254 SOL 0 0 f0100329 2970 kern/console.c 256 SOL 0 0 f010033a 2985 ./inc/x86.h 258 SOL 0 0 f0100340 2970 kern/console.c 260 SOL 0 0 f0100345 2985 ./inc/x86.h 262 SOL 0 0 f0100357 2970 kern/console.c 264 SOL 0 0 f010035a 2985 ./inc/x86.h 266 SOL 0 0 f010035d 2970 kern/console.c 268 SOL 0 0 f0100369 2985 ./inc/x86.h 270 SOL 0 0 f0100384 2970 kern/console.c 291 SOL 0 0 f01004ba 2985 ./inc/x86.h 293 SOL 0 0 f01004c2 2970 kern/console.c 295 SOL 0 0 f01004cc 2985 ./inc/x86.h 297 SOL 0 0 f01004e2 2970 kern/console.c 340 SOL 0 0 f01005b5 2985 ./inc/x86.h 342 SOL 0 0 f01005bd 2970 kern/console.c 344 SOL 0 0 f01005c0 2985 ./inc/x86.h objdump -G obj/kern/kernel | grep \"FUN\" 8 FUN 0 0 f0100040 2796 test_backtrace:F(0,20) 108 FUN 0 0 f0100094 2837 i386_init:F(0,20) 116 FUN 0 0 f01000f5 2855 _panic:F(0,20) 131 FUN 0 0 f010014c 2926 _warn:F(0,20) 174 FUN 0 0 f0100186 3012 serial_proc_data:f(0,1) 186 FUN 0 0 f01001a5 3036 cons_intr:f(0,20) 199 FUN 0 0 f01001e8 3091 kbd_proc_data:f(0,1) 244 FUN 0 0 f0100301 3137 cons_putc:f(0,20) 305 FUN 0 0 f01004ea 3173 serial_intr:F(0,20) 311 FUN 0 0 f0100506 3193 kbd_intr:F(0,20) 315 FUN 0 0 f0100518 3210 cons_getc:F(0,1) 329 FUN 0 0 f0100562 3227 cons_init:F(0,20) 369 FUN 0 0 f010066a 3274 cputchar:F(0,20) 374 FUN 0 0 f010067a 3300 getchar:F(0,1) 381 FUN 0 0 f010068b 3315 iscons:F(0,1) 429 FUN 0 0 f0100695 3981 mon_help:F(0,1) 436 FUN 0 0 f01006e4 4061 mon_kerninfo:F(0,1) 453 FUN 0 0 f0100794 4116 mon_backtrace:F(0,1) 477 FUN 0 0 f0100811 4170 monitor:F(0,20) 536 FUN 0 0 f0100945 4303 mc146818_read:F(0,4) 546 FUN 0 0 f010095c 4346 mc146818_write:F(0,20) 582 FUN 0 0 f0100973 4409 putch:f(0,20) 588 FUN 0 0 f0100986 4452 vcprintf:F(0,1) 598 FUN 0 0 f01009ac 4507 cprintf:F(0,1) 640 FUN 0 0 f01009c0 4684 stab_binsearch:f(0,20) 678 FUN 0 0 f0100ab6 4813 debuginfo_eip:F(0,1) 768 FUN 0 0 f0100d31 5135 printnum:f(0,20) 782 FUN 0 0 f0100de0 5253 getuint:f(0,9) 792 FUN 0 0 f0100e1a 5299 sprintputch:f(0,20) 802 FUN 0 0 f0100e37 5357 printfmt:F(0,20) 810 FUN 0 0 f0100e54 5422 vprintfmt:F(0,20) 918 FUN 0 0 f0101201 5529 vsnprintf:F(0,1) 936 FUN 0 0 f010124f 5595 snprintf:F(0,1) 974 FUN 0 0 f0101269 5699 readline:F(2,2) 1030 FUN 0 0 f0101342 5836 strlen:F(0,1) 1041 FUN 0 0 f010135a 5877 strnlen:F(0,1) 1051 FUN 0 0 f010137b 5915 strcpy:F(0,22)=*(0,2) 1060 FUN 0 0 f010139b 5985 strcat:F(0,22) 1071 FUN 0 0 f01013bd 6011 strncpy:F(0,22) 1083 FUN 0 0 f01013ea 6027 strlcpy:F(2,16) 1098 FUN 0 0 f0101425 6056 strcmp:F(0,1) 1110 FUN 0 0 f010144b 6110 strncmp:F(0,1) 1125 FUN 0 0 f0101483 6135 strchr:F(0,22) 还是能够发现很多整齐的用于调试的信息的。 首先分析这个二分的特点： 对某种类型的记录(第二列)进行查找的时候，内部有很多不同类型的记录，但是相同类型的地址n_value是单调增的。 尽管实验对二分的实现没有要求，我们可以进行一个简单的概括介绍： 首先初始化l=0, r=记录条数。 每次$mid = \\frac{l+r}{2}$，并且找到是这个类型的记录，因为中间混杂着很多不是该类型的，所以需要不断mid--。 然后l.n_value和r.n_value对比mid.n_value进行常规的放缩。 最后我们左端一定是这个类型，地址也是这个数值的记录，右端不一定。 最后给一个例子进行理解。 // For example, given these N_SO stabs: // Index Type Address // 0 SO f0100000 // 13 SO f0100040 // 117 SO f0100176 // 118 SO f0100178 // 555 SO f0100652 // 556 SO f0100654 // 657 SO f0100849 // this code: // left = 0, right = 657; // stab_binsearch(stabs, &left, &right, N_SO, 0xf0100184); // will exit setting left = 118, right = 554. // 最终的实现： 在kdebug.c中： stab_binsearch(stabs, &lline, &rline, N_SLINE, addr); info->eip_line = lline > rline ? -1 : stabs[rline].n_desc; monitor.c中： cprintf(\"Stack backtrace:\\n\"); uint32_t ebp, eip; struct Eipdebuginfo info; //指针学习 //+1实际上偏移的是4bytes for(ebp=read_ebp(); ebp!=0; ebp=*((uint32_t *)ebp)){ eip = *((uint32_t *)ebp+1); debuginfo_eip(eip, &info); cprintf(\" ebp %08x eip %08x args %08x %08x %08x %08x %08x\\n\", ebp, eip, *((uint32_t *)ebp+2), *((uint32_t *)ebp+3), *((uint32_t *)ebp+4), *((uint32_t *)ebp+5), *((uint32_t *)ebp+5), *((uint32_t *)ebp+6)); //.有截断的能力，*能够让eip_fn_namelen阶段字符串 cprintf(\" %s:%d: %.*s+%d\\n\", info.eip_file, info.eip_line, info.eip_fn_namelen, info.eip_fn_name, eip-info.eip_fn_addr); } 最后测试结果： 参考 AT&T interl汇编语言区别/内联汇编 Far jump in GDT in bootloader 如何从16位切换到32位系统 为什么计算机BIOS从0x7c00启动boot loader Control register printf变长参数实现的原理 "},"lab2-Memory Management.html":{"url":"lab2-Memory Management.html","title":"Lab2: Memory Management","keywords":"","body":"lab2: Memory Management 内存管理主要有下面两方面的功能： 内存分配，能够及时的分配和回收，知道哪些进程和内存相映射。 虚拟内存管理。 注意：特别留意本文页目录(page directory rentry)和页表(page table entry)。 本文主要围绕memlayout.h中的那张虚拟内存表和128MB实际物理内存映射进行展开，注意区别。 Part 1: Physical Page Management 这一部分需要做的是对物理内存的管理工作。 首先有函数i386_detect_memory()用来检查物理内存的大小。本实验检测出来能够使用的物理内存是128MB。并且是按照page为单元进行管理的，1 page=4096 Bytes。为什么是这个大小的空间进行内存的管理，其实和内存管理有关。一个32位的地址可以被这样 进行划分管理： // A linear address 'la' has a three-part structure as follows: // // +--------10------+-------10-------+---------12----------+ // | Page Directory | Page Table | Offset within Page | // | Index | Index | | // +----------------+----------------+---------------------+ // \\--- PDX(la) --/ \\--- PTX(la) --/ \\---- PGOFF(la) ----/ // \\---------- PGNUM(la) ----------/ // 可以看到，最后的12位就是页(page)内偏移，即2^12=4096Bytes大小。 exercise 1 在kern/pmap.c中，通过修改下面的函数，对物理内存进行管理： boot_alloc() // 分配n bytes大小的空间，空间大小以PAGE大小向上取整。 mem_init() (only up to the call to check_page_free_list(1)) page_init()：// 将可使用的内存以链表的形式连接在一起。 page_alloc(): // 分配一个空闲page page_free(): // 与上面的功能相反，收回一个空闲页。 boot_alloc(uint32_t n): 这个函数仅在系统建立虚拟内存映射系统的时候使用，后面所有的内存分配单位都是PAGE，并且使用page_alloc(尽管这个n也是向着PAGE大小为单位向上取整的。 因此我们的代码如下： // LAB 2: Your code here. result = nextfree; nextfree = ROUNDUP(nextfree+n, PGSIZE); if((uint32_t)nextfree-KERNBASE > (npages*PGSIZE)){ panic(\"OUT OF MEMORY\"); } return result; 之后，为了能够更好的管理物理内存，我们需要对每一个物理页有管理的元数据，使得更好的进行索引和权限的管理。 pages = (struct PageInfo *)boot_alloc(npages * sizeof(struct PageInfo)); memset(pages, 0, npages * sizeof(struct PageInfo)); 总共需要131072KB=128MB的物理内存需要管理，这个大小是i386_detect_memory()检测出来的。因此需要131072/4 = 32768个PageInfo进行管理，每一个PageInfo都会管理一个物理page，或者是管理PageInfo指向的物理page里面维护的内容是Page地址和权限。 物理内存的分布情况： page_init()函数主要是将物理内存中空闲的部分以链表的形式存储起来进行分配和回收。 JOS将内存分成了四个部分。 其中前1MB的内存分成了三个部分+1MB之后的内容： [0-4)KB：IDT表，BIOS structure （不可用） [4-640)KB: base memory （可用） [640,1024)KB:IO hole （不可用） —————————————————————————————— 1MB [1024, ...) （可用） 其中前面的1MB的内存是这样划分的，从lab1里面找来的结构： +------------------+ 最后我们内存的分配情况如下： 红色阴影部分的内存不可以再分配使用了。 因此我们对元数据pages的初始化和页空闲链表page_free_list进行初始化代码如下： size_t i; //boot_alloc(0)获取之前分配到的空闲页的首地址 const size_t pages_in_use_end = npages_basemem + 96 + ((uint32_t)boot_alloc(0) - KERNBASE) / PGSIZE; // pages_in_use_end = 600; cprintf(\"now in use: %d\\n\", pages_in_use_end); //设置让第0页为使用 cprintf(\"%08x %08x\\n\", pages, (uint32_t)boot_alloc(0)); //第0页用于存放real-mode IDT (interrupt descriptor table)and BIOS structures pages[0].pp_ref = 1; for (i = 1; i 其中96的由来就是(1024-640)/4=96。 page_alloc(): 每次调用分配一个page, 返回值为一个PageInfo。 代码如下： struct PageInfo * page_alloc(int alloc_flags) { // Fill this function in struct PageInfo *temp; if(page_free_list == NULL){ return NULL; } temp = page_free_list; page_free_list = temp->pp_link; temp->pp_link = NULL; if (alloc_flags & ALLOC_ZERO) //因为所有的程序中的地址都是虚拟地址进行操作的，所以我们需要将真实的物理页面转换到虚拟地址下初始化 memset(page2kva(temp), 0, PGSIZE); return temp; } page_free(): 回收相应的物理页面到空闲页链表，代码如下： void page_free(struct PageInfo *pp) { // Fill this function in // Hint: You may want to panic if pp->pp_ref is nonzero or // pp->pp_link is not NULL. if(pp->pp_ref != 0 || pp->pp_link != NULL) panic(\"can't properly free page\\n\"); pp->pp_link = page_free_list; page_free_list = pp; } Part 2: Virtual Memory exercise 2 阅读相关的文章，了解页转换机制和页的保护机制。 文章链接-i386设计 其中我们需要特别关注其中的两节，是我们之后实验的预备知识： Table translation Virtual, Linear, and Physical Addresses 这一部分讲到了虚拟地址是怎么进行的，我们只需要理解下面的图就行： 首先有一个寄存器CR3保存的是页目录的首地址，然后通过虚拟地址的高10位，即可找到页表开始位置。通过中间的10位+页表开始地址，即可找到具体的页地址。 其中DIR ENTRY32位我们都用来存PAGE TABLE的开始地址，由于一个页大小为4096Bytes，因此一共可以管理4096/4=1024项PAGE TABLE。PG TBL ENTRY我们的高20位用来存储PAGE FRAME ADDRESS，后面的12位用来维护具体PAGE的访问权限，如下图所示： 这也是page-level protection所重点说明的。 我们对其中的每一位进行具体的说明： P：PRESENT 当为0时，表示是一个无效的项： 一般我们的程序都会将其设置成1。 R/W: READ/WRITE, Read-only access (R/W=0), Read/write access (R/W=1) 当在内核态的时候，所有的pages都是可读可写的，当在用户态的时候，用户态的可读可写，内核态的仅可读。 关于怎么做大不同权限的访问，下面的U/S就能表示该页是内核态程序，还是用户态程序。当系统动态运行的时候，通过判断CPL寄存器的值，就能知道此刻用户的状态。从而能够进行不同用户的权限控制。 U/S：USER/SUPERVISOR, 目前该值的设置和CPL寄存器的值有关。若CPL=3，则是用户态，若是0，1，2则是内核态。 D：Dirty位是硬件进行设置的。主要是当内存满了，根据该位判断是否将这个页面重新写入硬盘中。若为1则肯定需要将其写入到硬盘中的。 Page Translation Cache 为了加速上面的虚拟地址的映射的过程，系统会将其放入到高速缓存中，每当这些表有变化的时候，需要手动的进行刷新，刷新的方法有下面的两种方法： 用EAX更新CR3 进程空间切换的时候也会进行这样的操作。 最后对应到程序中的位的设置： // Page table/directory entry flags. #define PTE_P 0x001 // Present #define PTE_W 0x002 // Writeable #define PTE_U 0x004 // User #define PTE_PWT 0x008 // Write-Through #define PTE_PCD 0x010 // Cache-Disable #define PTE_A 0x020 // Accessed #define PTE_D 0x040 // Dirty #define PTE_PS 0x080 // Page Size #define PTE_G 0x100 // Global exercise 3 Ctrl+a c 进入qemu的调试模式，使用xp/Nx address查看物理内存的内容，在GDB中查看虚拟地址的内存内容。 info mem info pg: 可查看页目录和页表的映射 info pg可以查看页目录和每一项页目录下对应页表下内容，如图所示： 我们可以看到初始化的时候我们仅初始化了第0x0和第0x3c0(960)页目录项。与entrypgdir.c里面的映射关系是一致的： __attribute__((__aligned__(PGSIZE))) pde_t entry_pgdir[NPDENTRIES] = { // Map VA's [0, 4MB) to PA's [0, 4MB) [0] = ((uintptr_t)entry_pgtable - KERNBASE) + PTE_P, // Map VA's [KERNBASE, KERNBASE+4MB) to PA's [0, 4MB) [KERNBASE>>PDXSHIFT] = ((uintptr_t)entry_pgtable - KERNBASE) + PTE_P + PTE_W }; 我们也观察到了960项多了一个PTE_W的权限，也是能够在截图中体现的。 但是，发现权限位多了很多的D和A的权限，怎么回事呢？ 我们可以查阅相关的资料，发现主要是用在多核系统中，系统在读或者是写时，会主动的更新这些位，当这些页面需要置换的时候，那么需要将置有D的page写回到硬盘中。 (qemu) info mem 0000000000000000-0000000000400000 0000000000400000 -r- 00000000f0000000-00000000f0400000 0000000000400000 -rw 其实有差异也是上面的初始化代码有关的。 我们将页目录项0增加一个权限+PTE_W，编译后的结果为： 确实是从这里进行权限的控制的。从这个角度来看，内存读取权限是4MB（一个页目录项管理的内存大小）来进行管理的。然而实际上应该是以4KB大小进行管理的。 一旦系统进入保护模式，所有的指针都是虚拟地址。 由于在内核初始化的时候，我们有的时候既需要操纵虚拟地址，有时候也需要操纵物理地址。 尽管我们在程序中只能使用虚拟地址，但是为了表示的方便性，我们使用了uintptr_t 和physaddr_t来区分虚拟地址和物理地址，尽管它们的数值类型一致。 Q: 一下代码x类型： mystery_t x; char* value = return_a_pointer(); *value = 10; x = (mystery_t) value; 是虚拟地址。 虚拟地址转物理地址没有什么好说的，有时候我们需要将物理地址转发为虚拟地址，比如： struct PageInfo * page_alloc(int alloc_flags) { // Fill this function in struct PageInfo *temp; if(page_free_list == NULL){ return NULL; } temp = page_free_list; page_free_list = temp->pp_link; temp->pp_link = NULL; if (alloc_flags & ALLOC_ZERO) memset(page2kva(temp), 0, PGSIZE); return temp; } 我们分配一个page的时候，能够知道其物理地址，但是因为只能操纵虚拟地址，于是我们我们又通过page2kva()将其转化为虚拟地址进行初始化。 Reference counting 主要是维护一个page被虚拟地址使用的次数。在后面的实验中，同一个物理地址(page)能够被多个虚拟地址同时映射，或者是多个用户态下的地址空间。 下面给出一个例子，从check_page()中摘选的片段进行说明： struct PageInfo *pp0, *pp1; assert((pp0 = page_alloc(0))); assert((pp1 = page_alloc(0))); assert(page_insert(kern_pgdir, pp1, 0x0, PTE_W) == 0); assert(page_insert(kern_pgdir, pp1, (void*) PGSIZE, PTE_W) == 0); assert(check_va2pa(kern_pgdir, 0) == page2pa(pp1)); assert(check_va2pa(kern_pgdir, PGSIZE) == page2pa(pp1)); assert(pp1->pp_ref == 2); 第一部分是初始化，第二部分分别是将0x0和0x400分别映射到了pp1。最后pp1被引用的次数就是2。 最后我们在内存中进行检查，发现确实如此： 0x3ff000是pp0页面的地址，里面存放的前两项就是pp1|PERM的值。 然后我们讨论一下页引用+1的操作都在哪些地方： pg directory entry若之前没有分配一个page，那么就会分配一个新的页，这个pg directory entry就会记录这个page的地址。 *pgdir_entry = (page2pa(new_page) | PTE_P | PTE_W | PTE_U); ++new_page->pp_ref; 每当虚拟地址和相关的页进行映射的时候，这个页的引用值会+1。 page_insert(pde_t *pgdir, struct PageInfo *pp, void *va, int perm){ ++pp->pp_ref; } Page Table Management exercise 4 建立虚拟地址和物理页之间的映射关系。 依次完成下面的函数： pgdir_walk() // boot_map_region() page_lookup() page_remove() page_insert() 前面我们主要维护了页的元数据PageInfo，并且将物理的页连接到了一起。下面，我们就需要将虚拟内存和物理内存建立联系。 pgdir_walk(pde_t *pgdir, const void *va, int create): 通过va高10位找到pg directory entry，若此时dir entry没有被初始化，则通过create来判断是否新建页。并且最后返回pg table entry。 pte_t * pgdir_walk(pde_t *pgdir, const void *va, int create) { // Fill this function in pde_t *pgdir_entry = pgdir + PDX(va); if (!(*pgdir_entry & PTE_P)) { if (!create) return NULL; else { struct PageInfo *new_page = page_alloc(1); if (!new_page) return NULL; *pgdir_entry = (page2pa(new_page) | PTE_P | PTE_W | PTE_U); ++new_page->pp_ref; } } return (pte_t *)(KADDR(PTE_ADDR(*pgdir_entry))) + PTX(va); } boot_map_region(kern_pgdir, 虚拟地址开始, 大小, 物理地址, 读取权限);：主要是通过地址区间，将虚拟地址和物理地址进行映射。 static void boot_map_region(pde_t *pgdir, uintptr_t va, size_t size, physaddr_t pa, int perm) { // Fill this function in int offset; pte_t *pgtable_entry; for (offset = 0; offset page_lookup(): 找到虚拟地址对应的页地址。 struct PageInfo * page_lookup(pde_t *pgdir, void *va, pte_t **pte_store) { // Fill this function in pte_t *pgtable_entry = pgdir_walk(pgdir, va, 0); if (!pgtable_entry || !(*pgtable_entry & PTE_P)) return NULL; if (pte_store) *pte_store = pgtable_entry; return pa2page(PTE_ADDR(*pgtable_entry)); } page_remove(): 将虚拟地址建立的映射关系取消。 void page_remove(pde_t *pgdir, void *va) { // Fill this function in pte_t *pgtable_entry; struct PageInfo *page = page_lookup(pgdir, va, &pgtable_entry); if (!page) return; page_decref(page); tlb_invalidate(pgdir, va); *pgtable_entry = 0; } page_insert(pde_t *pgdir, struct PageInfo *pp, void *va, int perm): 若该虚拟地址之前已经建立映射了，那么取消原来的映射关系（即将原来的pg table entry内容移除），并且将新的映射关系记录下来。 int page_insert(pde_t *pgdir, struct PageInfo *pp, void *va, int perm) { // Fill this function in pte_t *pgtable_entry = pgdir_walk(pgdir, va, 1); if (!pgtable_entry) return -E_NO_MEM; ++pp->pp_ref; if ((*pgtable_entry) & PTE_P) { tlb_invalidate(pgdir, va); page_remove(pgdir, va); } *pgtable_entry = (page2pa(pp) | perm | PTE_P); *(pgdir + PDX(va)) |= perm; return 0; return 0; } Part 3: Kernel Address Space JOS将32位的线性地址空间划分成了两个部分：用户环境和系统环境。它们之间的划分界限为ULIM。并且JOS为内核保存了256MB大小的空间（0xf0000000-0xffffffff=256MB，这就是说内核的空间了。注意不要和物理内存大小搞混了，实际上这个实验物理内存由QEMU进行设置的，与JOS无关，为128MB大小）。 Permissions and Fault Isolation [ULIM, ): 只有内核能够访问 [UTOP,ULIM): 都能够读，但是不能够write。实际上是内核向用户态主动暴露一些系统的信息，方便用户态进行读取和调用。 [0, UTOP): 用户态可以自由的进行权限的控制。 Initializing the Kernel Address Space exercise 5 完成mem_init() 里面函数功能，使得能够通过 check_kern_pgdir() 和check_page_installed_pgdir() 。 根据memlayout.h里面的虚拟内存的分布，进行相应的映射。 根据注释，我们进行下面的从虚拟地址到物理地址的映射操作： boot_map_region(kern_pgdir, UPAGES, PTSIZE, PADDR(pages), PTE_U); boot_map_region(kern_pgdir, KSTACKTOP - KSTKSIZE, KSTKSIZE, PADDR(bootstack), PTE_W); boot_map_region(kern_pgdir, KERNBASE, (0xffffffff-KERNBASE), 0, PTE_W); Q2：page directory 里面的1024项哪些被初始化过了，初始化后的值分别是什么？ UVPT = 0xef400000 = 957项 UPAGE = 0xef000000 = 956项 KSTACKTOP - KSTKSIZE = 0xefff 8000 = 959项 KERNBASE = 0xf0000000 = 960项 因此我们初始了上面的若干项，得到下面新的page directory表格： Entry Virtual Address Points to (logically) 1023 0xffc00000 ... 960 0xf0000000 KERNBASE 959 0xefff 8000 Kernel stack 957 0xef400000 Page directory 956 0xef000000 PageInfo array ... 0 0x00000000 NULL Q3: 我们将内核和用户态放在了同一个地址空间，为什么用户态不能读或者写内核的内存？是什么机制保证了能够这样做？ 用户态的程序不能够访问不带有PTE_U标志的页。这些权限访问的标志位在page table中。 Q4: 最大支持多大的物理内存？怎么算的？ 前面多次提到，最大为4GB。 Q5: 我们使用了多少的内存空间来管理内存？进行具体的说明。 1 page direstory+1024pages+0x100000 PageInfo = 4KB+4*1024KB+8192KB = 12292KB = 12MB左右 Q6: 从什么时候开始EIP从低地址(1MB)转向高地址(KERNBASE)？什么使得EIP既能够在低地址运行，又能够在高地址运行？为什么这个转化是必要的？ 执行了这个语句之后： mov $relocated, %eax f0100028: b8 2f 00 10 f0 mov $0xf010002f,%eax jmp *%eax f010002d: ff e0 jmp *%eax 这一点在lab1 exercise 7也做了详细的说明。 之所以既能够在高地址也能够在低地址执行，是因为高地址和低地址都通过一个表(lab1 中提到了)映射到了[0, 4MB)中。因为内核的代码都是在高地址进行的，这样能够留下更多的空间给用户态。 Address Space Layout Alternatives 本部分完全是拓展部分，怎样不使用内核代码在高地址，用户程序在低地址的结构，甚至内核空间是不固定的？有待进一步的去学习挖掘。 结果 遗留问题 Q：不同虚拟地址能够映射到同一物理地址？ 可以。上面已经给出具体的例子。新的问题，系统是怎么知道进行内容的替换的，用完后还能够替换回来？ 如果进行访存的操作，如果此刻虚拟地址指向的物理地址不是想要的内容，那么就进行刷盘的操作，否则就能够进行直接读取的操作。 /* * Virtual memory map: Permissions * kernel/user * * 4 Gig --------> +------------------------------+ * | | RW/-- * ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ * : . : * : . : * : . : * |~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~| RW/-- * | | RW/-- * | Remapped Physical Memory | RW/-- * | | RW/-- * KERNBASE, ----> +------------------------------+ 0xf0000000 --+ * KSTACKTOP | CPU0's Kernel Stack | RW/-- KSTKSIZE | * | - - - - - - - - - - - - - - -| | * | Invalid Memory (*) | --/-- KSTKGAP | * +------------------------------+ | * | CPU1's Kernel Stack | RW/-- KSTKSIZE | * | - - - - - - - - - - - - - - -| PTSIZE * | Invalid Memory (*) | --/-- KSTKGAP | * +------------------------------+ | * : . : | * : . : | * MMIOLIM ------> +------------------------------+ 0xefc00000 --+ = 0xf0000000-4096*1024 * | Memory-mapped I/O | RW/-- PTSIZE * ULIM, MMIOBASE --> +------------------------------+ 0xef800000 * | Cur. Page Table (User R-) | R-/R- PTSIZE * UVPT ----> +------------------------------+ 0xef400000 * | RO PAGES | R-/R- PTSIZE * UPAGES ----> +------------------------------+ 0xef000000 * | RO ENVS | R-/R- PTSIZE * UTOP,UENVS ------> +------------------------------+ 0xeec00000 * UXSTACKTOP -/ | User Exception Stack | RW/RW PGSIZE * +------------------------------+ 0xeebff000 * | Empty Memory (*) | --/-- PGSIZE * USTACKTOP ---> +------------------------------+ 0xeebfe000 * | Normal User Stack | RW/RW PGSIZE * +------------------------------+ 0xeebfd000 * | | * | | * ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ * . . * . . * . . * |~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~| * | Program Data & Heap | * UTEXT --------> +------------------------------+ 0x00800000 * PFTEMP -------> | Empty Memory (*) | PTSIZE * | | * UTEMP --------> +------------------------------+ 0x00400000 --+ * | Empty Memory (*) | | * | - - - - - - - - - - - - - - -| | * | User STAB Data (optional) | PTSIZE * USTABDATA ----> +------------------------------+ 0x00200000 | * | Empty Memory (*) | | * 0 ------------> +------------------------------+ --+ * * (*) Note: The kernel ensures that \"Invalid Memory\" is *never* mapped. * \"Empty Memory\" is normally unmapped, but user programs may map pages * there if desired. JOS user programs map pages temporarily at UTEMP. */ "},"Lab3-User Environments.html":{"url":"Lab3-User Environments.html","title":"Lab3: User Environments","keywords":"","body":"Lab3: User Environments 整个实验部分框架图： 系统调用是通过中断来进行的，syscall()从用户态到内核态的陷入过程： 目录/user和/lib是用户态的程序，/kern和/inc（/inc是内核态的函数库文件，供内核内部调用）是运行在内核中的程序。 Part A: User Environments and Exception Handling Environment State JOS中将thread和address space的概念具体化。其中thread具体指的就是保存下来的寄存器的值env_tf，address space是page directory(env_pgdir)。 Allocating the Environments Array exercise 1 按照之前实验的过程，我们需要为environment分配相应的管理元数据空间，并且对虚拟空间和物理页地址进行映射。根据/inc/memlayout.h中的分配对权限进行分分配，用户态仅可读。 envs = (struct Env*)boot_alloc(NENV*sizeof(struct Env)); memset(envs, 0, NENV*sizeof(struct Env)); boot_map_region(kern_pgdir, UENVS, PTSIZE, PADDR(envs), PTE_U); Creating and Running Environments 在之前的实验中，我们已经在boot loader中加载了一个内核二进制的文件，并且运行。 这一部分，我们就是从内核态加载一个二进制文件，在用户态运行。 exercise 2 在env.c文件中，我们需要完成下面的函数功能： env_init(): 初始化我们在exercise 1中分配的有关environments的元数据，将这些元数据通过env_free_list连接到一起。调用 env_init_percpu()，初始化一些段寄存器。 env_setup_vm(): 建立该environment的页目录管理空间，并且将内核的页目录管理内容复制到这个新创建的地方，将UVPY(Cur. Page Table part)虚拟地址这个部分的物理地址更改为当前物理页面的物理地址。 region_alloc(): 通过elf文件中的memsz和p_va申请若干的物理页，并且进行虚拟地址和物理地址的映射，保存到当前environment的页目录中。 load_icode(): 解析一个ELF文件，将其加载到user environment中，有点类似于加载内核的镜像ELF一样。 env_create(): 使用env_alloc()(从env_free_list中分配一个env 元数据变量)和load_icode() env_run(): 本实验的一个核心问题，怎么从内核态转换到用户态。 上面若干函数调用过程： env_init() env_create() env_alloc() env_setup_vm() load_icode() region_alloc() env_run() env_init()部分的实现思路及代码： 将空闲的envs链接起来 int counter; env_free_list = NULL; for (counter = NENV - 1; counter >= 0; --counter) { envs[counter].env_id = 0; envs[counter].env_status = ENV_FREE; envs[counter].env_link = env_free_list; env_free_list = &envs[counter]; } env_setup_vm(): 建立每一个进程的页目录管理。 ++p->pp_ref; e->env_pgdir = (pde_t *)page2kva(p); memcpy(e->env_pgdir, kern_pgdir, PGSIZE); // UVPT maps the env's own page table read-only. // Permissions: kernel R, user R e->env_pgdir[PDX(UVPT)] = PADDR(e->env_pgdir) | PTE_P | PTE_U; region_alloc(): 分配并且映射物理内存。 static void region_alloc(struct Env *e, void *va, size_t len) { // LAB 3: Your code here. // (But only if you need it for load_icode.) // // Hint: It is easier to use region_alloc if the caller can pass // 'va' and 'len' values that are not page-aligned. // You should round va down, and round (va + len) up. // (Watch out for corner-cases!) struct PageInfo *page = NULL; va = ROUNDDOWN(va, PGSIZE); void *end = (void *)ROUNDUP(va + len, PGSIZE); for (; va env_pgdir, page, va, PTE_U | PTE_W)) panic(\"region_alloc: page mapping failed.\"); } } load_icode(): 将二进制的程序加载到内存中 struct Elf *elf_header = (struct Elf *)binary; if (elf_header->e_magic != ELF_MAGIC) panic(\"load_icode: illegal ELF format.\"); lcr3(PADDR(e->env_pgdir)); struct Proghdr *ph = (struct Proghdr *)((uint8_t *)(elf_header) + elf_header->e_phoff); struct Proghdr *eph = ph + elf_header->e_phnum; for (; ph p_type == ELF_PROG_LOAD) { region_alloc(e, (void *)ph->p_va, ph->p_memsz); memmove((void *)ph->p_pa, binary + ph->p_offset, ph->p_filesz); memset((void *)(ph->p_pa + ph->p_filesz), 0, ph->p_memsz - ph->p_filesz); } } e->env_tf.tf_eip = elf_header->e_entry; lcr3(PADDR(kern_pgdir)); region_alloc(e, (void *)(USTACKTOP - PGSIZE), PGSIZE); 当上面的功能完成之后，内核会去执行一个hello的ELF文件，但是一旦执行到int指令的程序，就会有问题。 首先在这个节点上还没有任何硬件初始化来处理异常。 当内核发现没有建立异常处理机制时，会产生a double fault exception。 并且最终放弃，给出\"triple fault\"。 在env_pop_tf()函数中的iret指令后会从内核进入到用户模式，这个一个分界的指令。 说说这个iret的含义： 在一般的函数调用中，我们call address之后一定会有一个ret指令，该指令就是返回call下一条指令地址。 iret指的是中断的返回。我们之前都没有调用哪来的返回啊？ 这就是从内核态切换到用户态的精髓所在了。我们在之前的初始化程序中初始化了若干的寄存器，例如： //env_alloc() e->env_tf.tf_ds = GD_UD | 3; e->env_tf.tf_es = GD_UD | 3; e->env_tf.tf_ss = GD_UD | 3; e->env_tf.tf_esp = USTACKTOP; e->env_tf.tf_cs = GD_UT | 3; 我们看到我们初始化了environment的栈设置到了USTACKTOP。 我们首先将esp移动到了tf结构体，然后iret相当于是将栈中的内容赋值到了相应的寄存器中: 我们将相应的结构体和值相对应，发现确实是这样的。 struct Trapframe { struct PushRegs tf_regs; uint16_t tf_es; uint16_t tf_padding1; uint16_t tf_ds; uint16_t tf_padding2; uint32_t tf_trapno; /* below here defined by x86 hardware */ uint32_t tf_err; uintptr_t tf_eip; uint16_t tf_cs; uint16_t tf_padding3; uint32_t tf_eflags; /* below here only when crossing rings, such as from user to kernel */ uintptr_t tf_esp; uint16_t tf_ss; uint16_t tf_padding4; } __attribute__((packed)); 当我们执行完iret后，我们的esp寄存器确实在了USTACKTOP这里： 此刻我们在/lib/entry.S中。 知道了怎么从内核->用户态，那么我们怎么从用户态->内核态呢？ 我们知道，我们都是通过中断进入到内核态的，那么在中断之前，我们观察栈： 发现此时执行的代码依旧是在用户态（地址为0x800b2d较低）。 我们特意打印了一些寄存器的值，看看int后会将哪些寄存器的值压入。 我们发现了下面的压栈顺序： ss old esp eflags cs old eip 0(我也不知道是什么寄存器的值，可能是error code) 中断类型 并且此刻的栈值为0xefffffe8，查看memlayout.h发现就是CPU0的内核栈。 并且我们发现这个栈值并不是原来kernel所在stack的值，完全是一个不同的栈（CPU0的栈，内核栈从内核态切换到用户态一去不复返了）。这也是为什么软中断不能嵌套的一个原因，因为从用户态到内核态都是从KSTACKTOP往下改变的（后面我们会知道中断向量表中0-31号中断属于系统中断，应该是可以嵌套的）。 于是，我们终于搞懂了内核->用户态，用户态->内核态的全过程。 之后我们上面做的工作都应该正确直到执行int $30。确保上面工作的正常。 Handling Interrupts and Exceptions 阅读i386手册，去了解中断的原理 本实验中，exception, trap, interrupt, fault 和 abort没有本质的区别。 call和int的区别： 在调用call的过程中，会将call的下一条指令压入到栈中。（栈帧是连续的） 而在int 指令中，首先跳转到新的栈地址（栈帧是不连续的），依次压入原esp，eflags，cs，和int下一条指令地址。 本质上压栈的结构是这样的： +--------------------+ KSTACKTOP | 0x00000 | old SS | \" - 4 | old ESP | \" - 8 | old EFLAGS | \" - 12 | 0x00000 | old CS | \" - 16 | old EIP | \" - 20 如下面过程所示，将调用前和调用后的栈空间内容显示出来，并且这个内容在前面用户态->内核态的切换已经讲过了，主要就是想强调中断压栈顺序的重要性： 下面的两个机制保证了中断的有序进行： 中断向量表：x86中断向量表最多支持256个中断向量，为什么是怎么大也是系统的规定。主要在表中记录下面的两个值： EIP：中断发生了应该跳转到哪里 CS 寄存器的值，包括末两位指示优先级的值（JOS中，所有的中断都是在内核模式进行的，因此优先级=0） The Task State Segment(TSS)：系统需要一个地方来存储旧的处理器的状态。在JOS中，我们仅仅使用ESP0 和SS0，这也是为什么我们从用户态->内核态，栈寄存器一下子就变成了KSTACKTOP的原因，是因为TSS在从内核态->用户态的时候就已经保存好了内核的状态，方便能够返回内核态。 这个东西主要依靠tr寄存器的支持，tr寄存器的结构： 因此整个找到原来处理器状态的过程为，其中绿色的框表示系统中涉及到的初始化的部分： 我们初始化了tr寄存器的SELECTOR部分，从而能够通过GDT来找到TSS，找到TSS中的old ESP和CS就确定了原来栈的状况了，EIP和CS根据中断向量表来确定。 // 初始化部分 // Initialize the TSS slot of the gdt. gdt[GD_TSS0 >> 3] = SEG16(STS_T32A, (uint32_t) (&ts), sizeof(struct Taskstate) - 1, 0); gdt[GD_TSS0 >> 3].sd_s = 0; ltr(GD_TSS0); 综上，我们想要从用户态转移到内核态进行小心的处理，就需要借助中断向量表和TSS，从中断向量表中加载EIP和CS，从TSS中加载ESP和SS，从而就完成了用户态->内核态的转移工作。 GDT, IDT, LDT和TSS的关系 JOS所有的实验都没有使用LDT（local descriptor table）。它的作用是定位正在运行的处理器(processor)的segment，并且入口保存到了GDT，配合LDTR (Local Descriptor Table Register)一起使用。 GDT在JOS中主要是保存各段的位置等信息，配合若干寄存器使用如SS，TR(task register)等等。 IDT[256]中断向量表 TSS通过tr寄存器和GDT能够找到。 Basics of Protected Control Transfer Types of Exceptions and Interrupts 有0-31的系统内核内部的中断(the processor exceptions)。 可自由定义的软中断。 硬件中断。 其中第一种是可以嵌套的，其他两种应该不能进行嵌套。 一个中断发生的例子 Trapframe结构在栈中建立的过程，以int $0x30为例： int指令会从用户栈切换到内核栈KSTACKTOP。并且依次压入以下的值： (xxx)(目前未知) ss old esp eflags cs old eip error code /kern/trapentry.S中有下面的指令：pushl $(num); 现在的栈内容为： _alltraps执行完相应的压栈指令，内存为： 上面就构建了一个完整的Trapframe，为什么呢？我们再次对比Trapframe的结构体变量结构： struct Trapframe { struct PushRegs tf_regs; uint16_t tf_es; uint16_t tf_padding1; uint16_t tf_ds; uint16_t tf_padding2; uint32_t tf_trapno; /* below here defined by x86 hardware */ uint32_t tf_err; uintptr_t tf_eip; uint16_t tf_cs; uint16_t tf_padding3; uint32_t tf_eflags; /* below here only when crossing rings, such as from user to kernel */ uintptr_t tf_esp; uint16_t tf_ss; uint16_t tf_padding4; } __attribute__((packed)); 栈中存放的就是这个结构体啊。 总结构建按Trapframe的步骤就是： int $0x30 pushl $(num) pushl %ds pushl %es pushal //压入通用寄存器的值 构造完了Trapframe之后，会调用call trap，trap调用的参数如下：trap(struct Trapframe *tf)。这就进一步的验证了lab1中函数栈的结构问题了，再次放上图，并且在原来的图上增加了Trapframe的位置： 这就是整个trap()调用的机制了。 Nested Exceptions and Interrupts 若在用户态->内核态后，在内核态又发生了新的中断，那么栈是在原来的基础上进行改变的。 因为没有新的栈空间的转变，因此我们在调用中断的时候不需要压入old ss和old esp。 +--------------------+ KSTACKTOP | 0x00000 | old SS | \" - 4 | old ESP | \" - 8 | old EFLAGS | \" - 12 | 0x00000 | old CS | \" - 16 | old EIP | \" - 20 Setting Up the IDT exercise 4 完成idt的初始化 建立相应的Trapframe 实现的_alltraps 函数应该： 将值压入堆栈，使堆栈看起来像一个Trapframe 加载GD_KD 进入%ds 以及%es 使用pusl %esp 给Trapframe 传递指针，作为trap() 的参数 调用trap 通过上面的两个部分，我们就能够通过一些测试：divzero, softint, and badsegment 定义相应的处理函数 /* * Lab 3: Your code here for generating entry points for the different traps. */ //本部分是函数声明，并不是顺序执行的部分。 TRAPHANDLER_NOEC(handler_divide, T_DIVIDE) TRAPHANDLER_NOEC(handler_debug, T_DEBUG) TRAPHANDLER_NOEC(handler_nmi, T_NMI) TRAPHANDLER_NOEC(handler_brkpt, T_BRKPT) TRAPHANDLER_NOEC(handler_oflow, T_OFLOW) TRAPHANDLER_NOEC(handler_bound, T_BOUND) TRAPHANDLER_NOEC(handler_illop, T_ILLOP) TRAPHANDLER_NOEC(handler_device, T_DEVICE) TRAPHANDLER_NOEC(handler_simderr, T_SIMDERR) TRAPHANDLER_NOEC(handler_fperr, T_FPERR) TRAPHANDLER_NOEC(handler_mchk, T_MCHK) TRAPHANDLER_NOEC(handler_syscall, T_SYSCALL) TRAPHANDLER(handler_dblflt, T_DBLFLT) TRAPHANDLER(handler_tss, T_TSS) TRAPHANDLER(handler_segnp, T_SEGNP) TRAPHANDLER(handler_stack, T_STACK) TRAPHANDLER(handler_gpflt, T_GPFLT) TRAPHANDLER(handler_pgflt, T_PGFLT) TRAPHANDLER(handler_align, T_ALIGN) 在trap.c 中完成trap_init，对于系统的IDT 表进行初始化： //这些函数的实现都在trapentry.S中实现。 void handler_divide(); void handler_debug(); void handler_nmi(); void handler_brkpt(); void handler_oflow(); void handler_bound(); void handler_illop(); void handler_device(); void handler_simderr(); void handler_fperr(); void handler_mchk(); void handler_syscall(); void handler_dblflt(); void handler_tss(); void handler_segnp(); void handler_stack(); void handler_gpflt(); void handler_pgflt(); void handler_align(); void trap_init(void) { extern struct Segdesc gdt[]; // LAB 3: Your code here. SETGATE(idt[T_DIVIDE], 0, GD_KT, handler_divide, 0); SETGATE(idt[T_DEBUG], 0, GD_KT, handler_debug, 0); SETGATE(idt[T_NMI], 0, GD_KT, handler_nmi, 0); SETGATE(idt[T_BRKPT], 0, GD_KT, handler_brkpt, 3); SETGATE(idt[T_OFLOW], 0, GD_KT, handler_oflow, 0); SETGATE(idt[T_BOUND], 0, GD_KT, handler_bound, 0); SETGATE(idt[T_ILLOP], 0, GD_KT, handler_illop, 0); SETGATE(idt[T_DEVICE], 0, GD_KT, handler_device, 0); SETGATE(idt[T_SIMDERR], 0, GD_KT, handler_simderr, 0); SETGATE(idt[T_FPERR], 0, GD_KT, handler_fperr, 0); SETGATE(idt[T_MCHK], 0, GD_KT, handler_mchk, 0); SETGATE(idt[T_SYSCALL], 0, GD_KT, handler_syscall, 3); SETGATE(idt[T_DBLFLT], 0, GD_KT, handler_dblflt, 0); SETGATE(idt[T_TSS], 0, GD_KT, handler_tss, 0); SETGATE(idt[T_SEGNP], 0, GD_KT, handler_segnp, 0); SETGATE(idt[T_STACK], 0, GD_KT, handler_stack, 0); SETGATE(idt[T_GPFLT], 0, GD_KT, handler_gpflt, 0); SETGATE(idt[T_PGFLT], 0, GD_KT, handler_pgflt, 0); SETGATE(idt[T_ALIGN], 0, GD_KT, handler_align, 0); // Per-CPU setup trap_init_percpu(); } Q1: 为什么每一个中断都需要一个单独的中断函数进行处理？而不是放在一个函数中进行处理？ 有的函数需要返回，有的不需要，可能处理的级别也是不同的（有在用户态(如syscall)，有在内核态）因此分开处理较为的明确。当然没有一定的说法，如果一定要放在一个函数中处理也是可以的。 Q2: 在/user/softint.c中试图产生一个int $14的中断，但是在实际的过程中却产生的是13号中断general protection fault，这是为什么？如果允许用户产生14号中断，那么会发生什么？ 首先是在用户态不允许进行越权中断的，因此不论使用什么中断，都会产生13号中断。如果运行调用14号中断，相当于是允许在用户态调用内核态的中断，这很可能破坏内核中的状态。其实在用户态能有限的调用一些中断，从而能够从用户态陷入到内核态，比如handler_syscall和handler_brkpt。 综上，part A主要做的就是创建用户态environment，iret将内存中的值赋值到相应的寄存器中，从而能够从内核态切换到用户态。然后我们又了解了怎么从用户态切换到内核态的过程，栈中值是怎么变化的。并且这个Trapframe是我们之后从内核态返回用户态的关键，至于怎么返回，则和初次从内核态->用户态的方法是一致的。 Part B: Page Faults, Breakpoints Exceptions, and System Calls 在上面的中断的基础之上，我们需要实现更多的系统原语。 Handling Page Faults // buggy program - faults with a read from location zero #include void umain(int argc, char **argv) { cprintf(\"I read %08x from location 0!\\n\", *(unsigned*)0); } 我们看看这个中断是怎么触发的，在cprintf函数调用进行单步调试，得到下面的信息： 可以看到，在调用cprintf的第一个指令后就中断了，具体怎么触发的感觉不是很清楚，感觉应该是有一个地址检测的机制？（这里不是很懂） exercise 5 修改trap_dispatch()函数，使得发生页错误异常的时候触发page_fault_handler()。 switch(tf->tf_trapno){ case T_PGFLT: page_fault_handler(tf); return ; } The Breakpoint Exception 这一个功能就是实现我们平时断点的功能，实现的原理也很简单，就是在断点的语句前面加上int $3。 JOS中进入后就是进入monitor shell。 exercise 6 修改trap_dispatch，使断点异常发生时能够调用kernel monitor。修改完成后重新make grade 应该能够通过breakpoint 测试。 switch(tf->tf_trapno){ case T_PGFLT: page_fault_handler(tf); return ; case T_BRKPT: monitor(tf); return ; } Q3: 在breakpoint.c的的测试中，这个测试的例程要么会产生一个断点异常，要么会产生一个general protection fault的异常，产生哪种异常取决于如何初始化中断向量表IDT的。为什么？ SETGATE(idt[T_BRKPT], 0, GD_KT, handler_brkpt, 3);//产生Break point的中断 SETGATE(idt[T_BRKPT], 0, GD_KT, handler_brkpt, 0);//产生General Protection DPL 字段为段描述符优先级，如果当前程序为用户态但是尝试调用内核态的指令的时候就会触发general protection exception。只有当前程序的优先级小于或等于段描述符优先级才能触发正确的breakpoint exception。 Q4：你认为为什么要中断的机制，特别是考虑到user/softint测试例程中的行为？ 这样使得用户态不能随意的访问内核的代码和内存，使得内核不受用户程序的破坏。 System calls system call这一部分据说用户态能够从用户态陷入到内核态进行资源的调用。 JOS中，我们特别使用int $0x30进行调用， syscall函数输入: syscall number->%eax, 其余函数的变量放到 %edx, %ecx, %ebx, %edi, and %esi，最多支持五个变量。 并且最终的返回值保存到%eax中。 exercise 7 通过编辑kern/trapentry.S 以及kern/trap.c 的trap_init()，给T_SYSCALL 添加一个中断向量处理函数。同时trap_dispatch 也需要被修改，通过调用syscall 的方法来处理系统调用。最后，需要在kern/syscall.c 中首先实现syscall 函数。如果系统调用号不合法，需要syscall 返回-E_INVAL。 通过make run-hello 运行user/hello，qemu 应该打印处hello, world，并触发一个page fault。并且make grade 应该能够通过testbss 测试。 // kern/trapentry.S TRAPHANDLER_NOEC(handler_syscall, T_SYSCALL); // trap_init函数中添加handler_syscall SETGATE(idt[T_SYSCALL], 0, GD_KT, t_syscall, 3); // kern/trap.c case T_SYSCALL: tf->tf_regs.reg_eax = syscall( tf->tf_regs.reg_eax, tf->tf_regs.reg_edx, tf->tf_regs.reg_ecx, tf->tf_regs.reg_ebx, tf->tf_regs.reg_edi, tf->tf_regs.reg_esi); return ; // 然后在kern/syscall.c 中完成对于syscall 的实现，从而完成对于整个int 指令的调用： // Dispatches to the correct kernel function, passing the arguments. int32_t syscall(uint32_t syscallno, uint32_t a1, uint32_t a2, uint32_t a3, uint32_t a4, uint32_t a5) { // Call the function corresponding to the 'syscallno' parameter. // Return any appropriate return value. // LAB 3: Your code here. switch (syscallno) { case SYS_cputs: sys_cputs((char *)a1, a2); return 0; case SYS_cgetc: return sys_cgetc(); case SYS_getenvid: return sys_getenvid(); case SYS_env_destroy: return sys_env_destroy(a1); default: return -E_INVAL; } panic(\"syscall not implemented\"); } User-mode startup 运行相应的用户态程序。 如果在/kern/init.c中加入了ENV_CREATE(user_hello, ENV_TYPE_USER);，就能执行相应的语句，确保sys_getenvid()执行的正确性（这一步会进行中断调用）。 从内核启动用户态程序的过程： 根据前面的铺垫，首先是分配相应的进程空间，然后进行虚拟内存的映射，加载相关的二进制代码。当相关的内核状态初始化完成之后，就运行/lib/entry.S中的程序，这个过程运行在用户态，然后通过libmain.c通过调用umain()函数调用相关用户态程序。 exercise 8 Add the required code to the user library, then boot your kernel. You should see user/hello print \"hello, world\" and then print \"i am environment 00001000\". user/hello then attempts to \"exit\" by calling sys_env_destroy() (see lib/libmain.c and lib/exit.c). Since the kernel currently only supports one user environment, it should report that it has destroyed the only environment and then drop into the kernel monitor. You should be able to get make grade to succeed on the hello test. // /lib/libmain.c thisenv = envs + ENVX(sys_getenvid()); Page faults and memory protection 内存保护是一件非常重要的事情，使得一个程序的bugs不会干扰其他程序的内容或者正常的运行。 一般内存保护机制是硬件的支持，并且陷入到内核态。如果程序能够被修复，那么程序将会继续正常的执行，若无法修复，程序就无法正常的继续执行。 可修复的错误的例子：一开始内核给程序分配了一个函数栈，当调用的栈大小超过了指定大小时，报错陷入内核，内核会继续给该程序分配相应大小的内核栈，使得程序中断之后还能够继续执行。这一部分功能的实现在lab4中会进行具体的实现。 从用户态进行syscall时，陷入到内核态仍然需要对传入的参数进行解引用，相当于在用户态进行读写。这样做其实有两个问题： kernel page fault 和 user environment page fault两者的严重程度是不同的，在syscall调用造成的用户态page fault系统内核要能够正确的区分。 内核和用户态的权限往往是有很大的不同的，系统内核要能够进行区分。 因此，当在内核态遇到指针解引用时，要严格的对用户态传入的指针进行检查，如果是用户态的指针出现错误，那么就应该报出page fault，如果是内核中的指针出现错误，那么就应该panic and terminate，使得整个系统宕掉，因为这个是内核的错误，不能允许错误的出现。 exercise 9 修改kern/trap.c，如果在kernel mode发生 page fault, 那么就panic。 hint: 使用tf_cs来检测是在用户态还是在内核态 阅读/kern/pmap.c中的user_mem_assert，并且完成函数的功能的实现。 对system call的函数变量进行检查。 运行user/buggyhello能够正确的触发user_mem_check。 最后，在/kern/kdebug.c中更改debuginfo_eip, 该函数调用user_mem_check。之后运行user/breakpoint使得，陷入到monitor shell之后能够运行backtrace查看之前栈的情况，能够看到是/lib/libmain.c的调用信息。并且在traceback的时候会产生一个page fault。你不需要修复它，但是你要能够说明为什么会产生这样的错误。 首先判断错误是来自用户态还是来自内核态的： // Handle kernel-mode page faults. if (tf->tf_cs == GD_KT) panic(\"page_fault_handler: kernel page fault\"); // LAB 3: Your code here. user_mem_assert函数调用的是user_mem_check，它的作用是检查[va, va+len)这段用户态内存是否拥有perm|PTE_P权限。 int user_mem_check(struct Env *env, const void *va, size_t len, int perm) { // LAB 3: Your code here. uint32_t start = (uint32_t)ROUNDDOWN(va, PGSIZE); uint32_t end = (uint32_t)ROUNDUP(va + len, PGSIZE); pte_t *page; for (; start env_pgdir, (void *)start, 0); if (!page || start > ULIM || ((uint32_t)(*page) & perm) != perm ) { if (start 我们看到当运行用户态程序buggyhello.c中的程序的时候，需要输出相应的错误信息，发现调用的是sys_cputs()，因此该系统调用下进行相应的内存地址的判断： static void sys_cputs(const char *s, size_t len) { // Check that the user has permission to read memory [s, s+len). // Destroy the environment if not. // LAB 3: Your code here. user_mem_assert(curenv, s, len, 0); // Print the string supplied by the user. cprintf(\"%.*s\", len, s); } 最后修改kern/kdegbug.c 中的debuginfo_eip。添加如下代码： // Make sure the STABS and string table memory is valid. // LAB 3: Your code here. if(user_mem_check(curenv, usd, sizeof(struct UserStabData), PTE_U)) return -1; 在运行/user/breakpoint.c测试例程后，产生系统中断而进入monitor模式，此刻使用backtrace会打印一些错误的信息，但是这个时候还是会产生新的page fault，为什么呢？ 可以看到回溯的时候，用户态的栈指针越界了，从而会产生page fault的错误。 * USTACKTOP ---> +------------------------------+ 0xeebfe000 exercise 10 上面对用户传入内核指针地址的功能对恶意程序也同样适用，在运行user/evilhello.c中该程序尝试着打印内核的入口地址，也能正确的报错，并且内核不会panic。 本部分不需要进行任何的代码的实现，仅仅是测试样例的补充。 "},"lab4-Preemptive Multitasking.html":{"url":"lab4-Preemptive Multitasking.html","title":"Lab4: Preemptive Multitasking","keywords":"","body":"Lab 4: Preemptive Multitasking 这个实验中，我们将会实现一个抢占式(preemptive)的多核系统。 实验整体的思维导图： Part A: Multiprocessor Support and Cooperative Multitasking part A我们将拓展原来的JOS系统，使之变成一个多核系统。 之后实现一些system call，使得能够在用户态建立一些新的environments。 然后实现一个cooperative round-robin调度算法，当某个environment主动退出CPU的使用的时候，其他进程(后面默认进程 == environment)接着使用。 最后，我们会实现一个抢占式的调度算法，当某个进程执行一定的时间（不一定执行完了），使得内核能够重新使用CPU。 Multiprocessor Support 在启动的过程中，我们可以将CPU分成两类： the bootstrap processor (BSP)：主要负责内核的启动 the application processors(APs)：是被BSP启动的。 至于哪一个CPU核是BSP，则是被硬件和BIOS所确定的。 在SMP(symmetric multiprocessing)系统中，每一个CPU都会有一个local APIC(advanced programmable interrupt controller)。这些LAPIC主要负责传递一些中断信号。每一个单元都会有一个unique identifier。我们将会使用LAPIC单元的这些功能。 读取LAPIC id，从而知道我们现在此刻代码是跑在哪一个CPU中的。 uint8_t cpu_id; // Local APIC ID; index into cpus[] below BSP发送STARTUPinterprocessor interrrupt(IPI)给APs，使得能够启动其他的CPU。（见 lapic_startap()） 在part C，我们使用内置的时钟中断，使得能够支持抢占式多任务。 一个处理器通过memory-mapped I/O(MMIO)来获取它的LAPIC，相当于是直接通过读写内存来操控硬件（虽然看起来像是操作内存，但实际上还是和硬件中的存储进行交互）。之前我们知道在最开始的1MB内存中0xA0000的物理地址开始存放的是VGA display buffer。 exercise 1 实现kern/pmap.c中的mmio_map_region。 我们读取到CPU的配置信息后，里面有LAPIC单元的物理地址(lapicaddr)，我们需要做的就是将虚拟地址MMIOBASE映射到这一部分的物理地址。 void * mmio_map_region(physaddr_t pa, size_t size) { // Where to start the next region. Initially, this is the // beginning of the MMIO region. Because this is static, its // value will be preserved between calls to mmio_map_region // (just like nextfree in boot_alloc). // 这个static很关键啊，这样就不用保存为全局变量了。 static uintptr_t base = MMIOBASE; // write through: 同时更新cache和后端的存储中的数据 // write back: 仅仅更新cache中的数据，必要时才往后端的设备写回数据 // Your code here: if(base + ROUNDUP(size, PGSIZE) > MMIOLIM) panic(\"mmio_map_region: mmio overflow.\"); boot_map_region(kern_pgdir, base, ROUNDUP(size, PGSIZE), pa, PTE_W|PTE_PCD|PTE_PWT); uintptr_t temp = base; base += ROUNDUP(size, PGSIZE); return (void *)temp; panic(\"mmio_map_region not implemented\"); } mmio是一块很特殊的内存区域，能够像访问内存一样，去访问硬件的寄存器，但是又和一般的内存有不同的地方，一般内存都带有缓存的功能，也就是将内存的内容缓存到CPU的cache中。由于设备寄存器的多变性，我们需要将内存的缓存功能关闭。幸运的是，内存的权限位提供这样的支持，需要将相应的权限为设置为PTE_PCD和PTE_PWT即可，也就是cache disable和write through。 Application Processor Bootstrap 在启动APs，我们需要读取从BIOS区域读取一些信息，如CPU的数量，APIC ID和MMIO物理地址。 驱动APs的函数在boot_aps中。APs的启动在实模式下，和boot loader的启动方式很像。因此boot_aps()将要执行的代码复制到实模式能够寻址到的地方（实际上是放在了0x7000 (MPENTRY_PADDR)）。实际上任何放在640KB以下的内存都是没有问题的。 // Write entry code to unused memory at MPENTRY_PADDR code = KADDR(MPENTRY_PADDR); memmove(code, mpentry_start, mpentry_end - mpentry_start); 放置好要执行的代码之后，boot_aps依次激活APs，首先发送一个STARTUP的IPI中断，然后设置CS:IP寄存器。 之后APs类似于boot loader部分执行初始化代码，然后运行mp_main()初始化一些寄存器的值，如GDT，TSS等等。BSP会等待APs的 CPU_STARTED信号，收到了才会激活其他的APs。 exercise 2 阅读相关部分的代码，修改kern/pmap.c中的page_init函数，将MPENTRY_PADDR的页从page_free_list中移除。 for (i = 1; i Q1: 对比kern/mpentry.S和boot/boot.S，记住kern/mpentry.S始终是运行在KERNBASE地址之上的。那么在kern/mpentry.S中的MPBOOTPHYS宏定义的目的是什么？为什么在kern/mpentry.S是必要的，而在boot loader中不需要？如果去掉有什么错误？ A：boot.S在实模式下的，而重新进入时，mpentry.S是在保护模式下进行的。因此我们必须重新将其转化为实际的物理内存地址。 Per-CPU State and Initialization 当实现一个多核系统的时候，区分私有的状态还是共享的状态是非常重要的。 我们需要知道每一个CPU下面的一些状态： per-CPU kernel stack 每一个CPU的栈内容保存在percpu_kstacks[NCPU][KSTKSIZE]。我们需要将虚拟地址从KSTACKTOP开始向下依次的进行映射。 并且不同CPU栈中间是有一块隔离区的。 per-CPU TSS 和 TSS descriptor 前面我们提到TSS结构主要是保存处理器旧的状态，方便用户态跳转回内核态。现在多核系统中是保存CPU的栈和若干寄存器的状态，具体使用cpus[i].cpu_ts变量。TSS descriptor保存在个GDT中，通过gdt[(GD_TSS0 >> 3) + i]进行索引。 per-CPU目前执行的进程 通过cpus[cpunum()].cpu_env进行索引。 per-CPU system registers 所有的寄存器，包括系统寄存器，对CPU都是隐秘的（感觉这里讲的不清楚，应该每一个CPU都有属于自己的寄存器，相互独立不能访问，因此需要进行初始化）。因此我们需要使用一些函数lcr3(), ltr(), lgdt(), lidt()进行初始化。 exercise 3 修改在kern/pmap.c中的mem_init_mp()，从而将虚拟地址与物理地址进行相应的映射。 mem_init_mp(void) { // Map per-CPU stacks starting at KSTACKTOP, for up to 'NCPU' CPUs. // // For CPU i, use the physical memory that 'percpu_kstacks[i]' refers // to as its kernel stack. CPU i's kernel stack grows down from virtual // address kstacktop_i = KSTACKTOP - i * (KSTKSIZE + KSTKGAP), and is // divided into two pieces, just like the single stack you set up in // mem_init: // * [kstacktop_i - KSTKSIZE, kstacktop_i) // -- backed by physical memory // * [kstacktop_i - (KSTKSIZE + KSTKGAP), kstacktop_i - KSTKSIZE) // -- not backed; so if the kernel overflows its stack, // it will fault rather than overwrite another CPU's stack. // Known as a \"guard page\". // Permissions: kernel RW, user NONE // // LAB 4: Your code here: uint32_t i=0; uintptr_t start = KSTACKTOP-KSTKSIZE; for(; i KSTKGAP起到的作用就是隔离各CPU的栈空间，防止相互干扰。 exercise 4 在lab3 中，我们使用的是全局的ts来保存旧的一个处理器的状态。现在我们对每一个CPU都保存一个ts。 // Initialize and load the per-CPU TSS and IDT void trap_init_percpu(void) { // Setup a TSS so that we get the right stack // when we trap to the kernel. //ts.ts_esp0 = KSTACKTOP; //ts.ts_ss0 = GD_KD; //ts.ts_iomb = sizeof(struct Taskstate); struct Taskstate *thists = &thiscpu->cpu_ts; thists->ts_esp0 = KSTACKTOP - thiscpu->cpu_id * (KSTKSIZE + KSTKGAP); thists->ts_ss0 = GD_KD; thists->ts_iomb = sizeof(struct Taskstate); // Initialize the TSS slot of the gdt. gdt[(GD_TSS0 >> 3) + thiscpu->cpu_id] = SEG16(STS_T32A, (uint32_t) (thists), sizeof(struct Taskstate) - 1, 0); gdt[(GD_TSS0 >> 3) + thiscpu->cpu_id].sd_s = 0; // Load the TSS selector (like other segment selectors, the // bottom three bits are special; we leave them 0) ltr(GD_TSS0+ (thiscpu->cpu_id Locking 在让APs进一步执行前，我们需要解决内核竞争的问题防止多个CPU同时运行内核代码。 目前设置一个大锁，使得有一个CPU进入内核时，那么就锁住内核，仅允许一个CPU运行。当返回到用户态时，就释放该锁。 因此，上面的大锁设计，使得用户态的程序能够多CPU的运行，仅能有一个进程运行内核，其他的处理器进程都得等待。 我们需要在下面的四个地方运用内核锁： i386_init()中唤起其他APs前需要锁定内核 mp_main()中尝试着索取内核锁，从而能够进行进程的调用。 trap()中从用户态陷入到内核态，尝试着去获取锁。 env_run()在env_pop_tf()前释放锁。 我们看到仅有一处是锁的释放的，那就是从内核态进入到用户态的前夕。 exercise 5 在合适的位置运行lock_kernel和unlock_kernel。 目前还无法测试锁的正确性。 几处加锁： 一个CPU尝试着启动其他的CPU的时候 // Lab 4 multiprocessor initialization functions mp_init(); lapic_init(); // Lab 4 multitasking initialization functions pic_init(); // Acquire the big kernel lock before waking up APs // Your code here: lock_kernel(); // Starting non-boot CPUs, mpentry.S 入点 boot_aps(); 从用户态陷入到内核态： if ((tf->tf_cs & 3) == 3) { // Trapped from user mode. // Acquire the big kernel lock before doing any // serious kernel work. // LAB 4: Your code here. lock_kernel(); assert(curenv); } 一处解锁： 从内核到用户态： void env_run(struct Env *e) { curenv->env_status = ENV_RUNNING; ++curenv->env_runs; lcr3(PADDR(curenv->env_pgdir)); unlock_kernel(); env_pop_tf(&curenv->env_tf); } Q2： 目前的锁机制，使得每一次只有一个CPU运行内核代码，为什么我们仍然需要将CPU栈分离开？如果不分开有什么错？ A：lab3中，内核态->系统态一定是从栈的最顶端开始的(KSTACKTOP)，因此每一次陷入的时候都像是一个新的从未使用的内核栈。这样看的话，好像是能够进行公用的。但是不要忘了朋友们，后面的抢占式进行调度，那么内核栈中和有可能保存着我们需要的信息，或者是上次未执行完的信息。如果调用system call那么压入的参数就不一样啊，显然不能使用同一个栈。 Round-Robin Scheduling Round-Robin就是循序的进行调度，每一个进程调度的都是均等的。 执行的步骤如下： sched_yield()挑选一个进程进行执行，通过环形的方式对envs[]进行访问，挑选第一个状态为RUNNABLE的进程放入到目前CPU中进行执行。 sched_yield()不能在两个CPU上运行同一个进程。 系统已经提供了一个新的系统调用sys_yield()，使得用户态进程能够主动的调用sched_yeild()并且让其他的进程被CPU执行。 exercise 6 完成sched_yield()函数的实现，实现Round-Robin调度 首先完成调度的程序： // Choose a user environment to run and run it. void sched_yield(void) { struct Env *idle; // Implement simple round-robin scheduling. // // Search through 'envs' for an ENV_RUNNABLE environment in // circular fashion starting just after the env this CPU was // last running. Switch to the first such environment found. // // If no envs are runnable, but the environment previously // running on this CPU is still ENV_RUNNING, it's okay to // choose that environment. // // Never choose an environment that's currently running on // another CPU (env_status == ENV_RUNNING). If there are // no runnable environments, simply drop through to the code // below to halt the cpu. // LAB 4: Your code here. int counter; if (curenv) { for (counter = ENVX(curenv->env_id) + 1; counter != ENVX(curenv->env_id); counter = (counter + 1) % NENV){ //cprintf(\"%d\\n\", counter); if (envs[counter].env_status == ENV_RUNNABLE){ env_run(envs + counter); } } if(curenv->env_status != ENV_NOT_RUNNABLE) env_run(curenv); //cprintf(\"%d\\n\", counter); } else { for (counter = 0; counter 然后，在syscall 中添加一个case 来使用sys_yield 系统调用： case SYS_yield: sys_yield(); return 0; 之后在i386_init()函数中加入以下的代码： ENV_CREATE(user_yield, ENV_TYPE_USER); ENV_CREATE(user_yield, ENV_TYPE_USER); ENV_CREATE(user_yield, ENV_TYPE_USER); 输入make qemu CPUS=2就能看到下面的输出结果了： Hello, I am environment 00001000. Hello, I am environment 00001001. Hello, I am environment 00001002. Back in environment 00001000, iteration 0. Back in environment 00001001, iteration 0. Back in environment 00001002, iteration 0. Back in environment 00001000, iteration 1. Back in environment 00001001, iteration 1. Back in environment 00001002, iteration 1. ... Q3：在env_run()中使用了lcr3()用来更新页目录，因此MMU内容会立刻被更新。那么为什么更新前后当前准备运行的environment虚拟地址映射到物理地址没有变化？ 因为在进程页目录初始化时，复制的就是内核的页目录： e->env_pgdir = (pde_t *)page2kva(p); memcpy(e->env_pgdir, kern_pgdir, PGSIZE); 仅仅在UVPT这个虚拟地址有了修改，具体可以看看env_create.env_alloc.env_setup_vm 这个问题应该在lab3如何初始化environment就能够回答。 Q4：当CPU从一个environment转移到了另外一个environment，系统必须要保存旧的environment的寄存器使得它之后能够正确的被再次唤起。这个是在哪里进行的。 在env->env_tf中保存的，也就是Trapframe结构。保存是发生在_alltraps构造Trapframe，我们已经在lab3中详细的讨论过了。恢复发生在kern/env.c 中的env_pop_tf处。 System Calls for Environment Creation 现在能进行进程的切换，但是运行的进程数在系统初始化的时候就已经确定好了。下面的实现就是为了能够在用户态创建新的environment。 Unix使用fork()来进行创建。该函数会赋值整个进程的地址空间作为child process。parent process和child process的区别就是process ID。在parent进程中，fork返回child ID，在child process中，fork返回0(environment->env_tf.tf_regs.reg_eax = 0;)。 在JOS中，我们需要实现下面的system call: sys_exofork: 这个系统调用会产生一个空白的进程空间。 调用的parent env会获得子进程的进程号，而子进程得到的数值为0。 sys_env_set_status： 当初始化好了若干的页面的设置，那么将进程的状态ENV_NOT_RUNNABLE改位ENV_RUNNABLE。 sys_page_alloc: 根据相应的虚拟地址，分配相应的物理内存。 sys_page_map: 感觉是一种内存共享的方式，两个进程的页目录映射有同一块地址空间。 sys_page_unmap: 与上面的相反。 上面的所有系统调用函数参数中都会包括environment IDs。JOS提供ID到environment的转换，使用envid2env()。 特别需要注意的是，envid2env(0)返回的是当前environment的指针。 JOS此刻已经提供了一个简陋的像fork()那样的实现dumbfork()（为什么说简陋因为没有copy on write的机制）。我们需要实现上面的system call使得能够实现这个简陋的dumbfork()。 exercise 7 实现上面的system call。 当调用envid2env函数的时候，checkperm=1，使得我们始终检查environment关系。 检查所有的系统调用，如果参数不正确或者不在规定的范围，那么就返回-E_INVAL。 首先实现sys_exofork(): // Allocate a new environment. // Returns envid of new environment, or env_id)) env_status = ENV_NOT_RUNNABLE; environment->env_tf = curenv->env_tf; environment->env_tf.tf_regs.reg_eax = 0;//这应该就是子进程的返回值了 return environment->env_id; //panic(\"sys_exofork not implemented\"); } 之后实现sys_page_alloc(): static int sys_page_alloc(envid_t envid, void *va, int perm) { // Hint: This function is a wrapper around page_alloc() and // page_insert() from kern/pmap.c. // Most of the new code you write should be to check the // parameters for correctness. // If page_insert() fails, remember to free the page you // allocated! // LAB 4: Your code here. if(!(perm & PTE_U) || !(perm & PTE_U) || (perm & (~PTE_SYSCALL)) || va > (void *)UTOP || va != ROUNDDOWN(va, PGSIZE)) return -E_INVAL; struct PageInfo *pginfo = page_alloc(ALLOC_ZERO); if(!pginfo) return -E_NO_MEM; struct Env *environment; if(envid2env(envid, &environment, 1) env_pgdir, pginfo, va, perm) sys_page_map实现： static int sys_page_map(envid_t srcenvid, void *srcva, envid_t dstenvid, void *dstva, int perm) { // Hint: This function is a wrapper around page_lookup() and // page_insert() from kern/pmap.c. // Again, most of the new code you write should be to check the // parameters for correctness. // Use the third argument to page_lookup() to // check the current permissions on the page. // LAB 4: Your code here. if((uint32_t)srcva >= UTOP || PGOFF(srcva) || (uint32_t)dstva >= UTOP || PGOFF(dstva) || !(perm & PTE_U) || (perm & (~PTE_SYSCALL)) ) return -E_INVAL; struct Env *src_environemt, *dst_environment; if(envid2env(srcenvid, &src_environemt, 1) env_pgdir, srcva, &pte); // if(srcenvid == 4097) // *pte |= PTE_W; if(!page || (!(*pte & PTE_W) && (perm & PTE_W))) return -E_INVAL; if(page_insert(dst_environment->env_pgdir, page, dstva, perm) sys_page_unmap: static int sys_page_unmap(envid_t envid, void *va) { // Hint: This function is a wrapper around page_remove(). // LAB 4: Your code here. if((uint32_t)va >= UTOP || PGOFF(va)) return -E_INVAL; struct Env *environment; if(envid2env(envid, &environment, 1) env_pgdir, va); return 0; //panic(\"sys_page_unmap not implemented\"); } 最后增加这几个系统调用的分发： case SYS_exofork: return sys_exofork(); case SYS_env_set_status: return sys_env_set_status(a1, a2); case SYS_page_alloc: return sys_page_alloc(a1, (void *) a2, a3); case SYS_page_map: return sys_page_map(a1, (void *) a2, a3, (void *) a4, a5); case SYS_page_unmap: return sys_page_unmap(a1, (void *) a2); Part B: Copy-on-Write Fork Unix提供fork()来在用户态进行进程的创建。 vx6通过赋值所有parent的页进入新的页，从而创建新的进程。并且这样的复制行为代价也是整个fork()开销最大的操作。 然而，一个fork()函数调用后往往跟随着一个exec()函数，又需要将原来的内存替换为其他内容。因为child process对parent process拷贝后的内容用的非常的少，因此将整个页内容都拷贝过来将是是非浪费效率的一种行为。 正是上面遇到的这种问题，之后的unix利用虚拟内存的硬件，使得parent和child process能够共享这一部分的内存，直到其中的一个进程修改了页的内容，就会结束共享页的行为，这种行为叫做copy-on-write。 为了能够实现上面的功能，fork()实现时，内核将会赋值parent的address space mappings，也就是页目录和页表到child process，而不将内容拷贝到child process中，并且将内容权限设置为read-only。当其中的一个进程尝试对内存进行写的操作的时候，那么将会发生page falut中断。并且分配新的页，这些内容设置为可写。 上面描述的方式使得一般fork()操作的开销非常的小，一般只用复制1page（4096Bytes）。 User-level page fault handling 为了能够实现上面的copy-on-write，我们首先需要知道在read-only的页上会发生page fault的错误。 并且发生page fault这种时间也是非常常见的，比如一般内核在初始化程序的时候，仅会分配一个页，当栈空间不够的时候，那么就会发生page fault。同理，这些事件也会发生在BSS区域，并且初始化的时候会全部赋值为0。当需要执行的指令不在内存的时候，也会发生page fault，从而能够从磁盘读取相关指令。可见page fault是一个非常常见的事件，并且是一个很好优化系统性能的手段。 JOS中，我们并不会在内核态封装page fault handler，相反，我们在用户态可以自由的定制page fault handler。我们需要好好的设计page fault的处理机制，从而能够灵活的处理上面提到的多种发生page fault情况的事件。 之后我们需要处理获取从硬盘上的文件系统的内容。 Setting the Page Fault Handler 为了能够用户态有自己的page fault handler，我们需要在JOS内核的page fault handler entrypoint进行赋值确定（就是函数指针的赋值）。用户态进程通过sys_env_set_pgfault_upcall注册自己的系统调用。并且在Env结构体中增加了一个新的变量env_pgfault_upcall用来记录这个值。 exercise 8 实现sys_env_set_pgfault_upcall，确保权限的检查，使得只有相应的进程ID才能进行修改，因为这是一个非常危险的系统调用。 在kern/syscall.c中进行实现： // Set the page fault upcall for 'envid' by modifying the corresponding struct // Env's 'env_pgfault_upcall' field. When 'envid' causes a page fault, the // kernel will push a fault record onto the exception stack, then branch to // 'func'. // // Returns 0 on success, env_pgfault_upcall = func; return 0; // panic(\"sys_env_set_pgfault_upcall not implemented\"); } 并且在/kern/syscall.c/syscall()中进行分发： case SYS_env_set_pgfault_upcall: return sys_env_set_pgfault_upcall(a1, (void *) a2); Normal and Exception Stacks in User Environments 正常用户态的栈空间在[USTACKTOP-1, USTACKTOP-PGSIZE]之间。 当用户态产生中断，内核将会在一个设定好的栈空间重启用户进程，也就是在user exception stack进行处理。并且这个过程和用户态切换到内核态原理基本是相同的。 user exception stack也是一个页的大小，并且栈底的位置在UXSTACKTOP。然后在这个栈空间，能够正常的调用系统调用，从而能够对页进行重新的映射，分配或者是修复任何与page fault有关问题。user-level page handler通过汇编语言stub返回相应的值，也是原来栈上面填错误码(uint32_t tf_trapno)的地方。 每一个进程如果想要支持用户态的page fault handler，必须自己分配相应的异常处理栈，可以使用sys_page_alloc()进行分配。 Invoking the User Page Fault Handler 构造page fault handler栈如下面结果所示： 与/inc/trap.h里面结构体变量结构一致： struct UTrapframe { /* information about the fault */ uint32_t utf_fault_va; /* va for T_PGFLT, 0 otherwise */ uint32_t utf_err; /* trap-time return state */ struct PushRegs utf_regs; uintptr_t utf_eip; uint32_t utf_eflags; /* the trap-time stack to return to */ uintptr_t utf_esp; } __attribute__((packed)); 当中断处理未完成时，可以进行嵌套的操作，不过此时栈的变化是从此刻的tf->tf_eps向下增长的，而不是从UXSTACKTOP开始向下变化，反正是可以进行嵌套处理的。 确保异常处理栈不能超过空间大小，因为这个栈的下面就是用户进程栈，如果覆盖了那么即使异常正确的处理了，返回后程序依旧不能够正确的执行。 exercise 9 实现/kern/trap.c中的page_fault_handler，该函数能够分发用户态的page fault handler。 确保正确的对栈空间进行操作 如果 exception stack使用的空间超过了会发生什么？（上面回答了） 通过该函数的注释，我们需要对page fault进行分类讨论处理： 当初次发生page fault handler时，这个栈陷入的变化时用户栈->内核栈->异常处理栈（返回时异常栈->直接用户栈）。 当有page fault handler的嵌套时，此时已经在异常处理栈了，此时需要再次压入一个UTrapframe，并且之间空4Bytes，此时栈陷入顺序为：异常处理栈->内核栈->异常处理栈（注意返回顺序并不是这样的，而是直接异常栈->一场栈）。 并且我们要时刻保证压入的UTrapgrame没有超过内存限制大小，最终的实现代码如下： // LAB 4: Your code here. if(curenv->env_pgfault_upcall){ struct UTrapframe *utf; uintptr_t addr; // addr of utf if(UXSTACKTOP - PGSIZE tf_esp && tf->tf_esp tf_esp - sizeof(struct UTrapframe) - 4; else addr = UXSTACKTOP - sizeof(struct UTrapframe); user_mem_assert(curenv, (void *)addr, sizeof(struct UTrapframe), PTE_W); utf = (struct UTrapframe *)addr; utf->utf_fault_va = fault_va; utf->utf_err = tf->tf_err; utf->utf_regs = tf->tf_regs; utf->utf_eip = tf->tf_eip;//用户能够返回到用户态的设置 utf->utf_eflags = tf->tf_eflags; utf->utf_esp = tf->tf_esp; tf->tf_eip = (uint32_t)curenv->env_pgfault_upcall; tf->tf_esp = addr; env_run(curenv); } User-mode Page Fault Entrypoint 接下来，我们需要实现汇编代码，来对page fault正确的进行处理，并且处理完后能够正确的返回当前函数执行的位置。这个assembly routine将会在sys_env_set_pgfault_upcall()上register（赋值）上。 exercise 10 实现lib/pfentry.S中的_pgfault_upcall函数。 这一部分比较有趣的是能够返回用户态异常地址的地方，不需要通过内核进行返回。（注意陷入和返回的过程是不一样的） 最难的部分是同时变换栈和重新加载EIP。（此处同时变换不是指内存变换，而是再还原的过程中，有的寄存器一旦被还原，就不能再继续被使用了） 关于调用page_fault_handler过程比较的简单： 用UTrapframe来保存当前用户态的寄存器的值，这部分的值存储在异常栈处理部分UXSTACKTOP。 更新用户态的Tramframe中的EIP=page_fault_handler和ESP(异常栈部分) _pgfault_upcall调用用户自己定义的page_fault_handler，其中传入参数UTrapframe(pushl %esp)。 处理完后，我们直接从异常栈跳转到用户态（而不用通过内核态进行中转）。 这个跳转的部分就是这个实现比较难的部分了，下面讲讲具体的思路。 假设现在异常栈只压入了一个栈帧，此刻的栈长成这样： 首先执行因为栈顶的两个数据并不影响寄存器的值，不需要进行还原，addl $0x8, %esp,栈内容变成这样: 之后执行下面语句： subl $0x4, 0x28(%esp) movl 0x28(%esp), %edx 栈中的内容变成了这样： 运行下面的指令： movl 0x20(%esp), %eax movl %eax, (%edx) 栈中内容变成了： 经历下面的指令： popal addl $0x4, %esp popfl popl %esp 栈中的内容变成了： 最后执行ret，将栈顶的内容赋值给EIP，最终完成了跳转。 异常栈在分配空间的时候，会额外的多分配4Bytes，之前一直以为起到的是隔离的作用，不过最后才发现为了ret 给EIP赋正确的值。 最终合起来的代码： addl $0x8, %esp subl $0x4, 0x28(%esp) movl 0x28(%esp), %edx movl 0x20(%esp), %eax movl %eax, (%edx) popal addl $0x4, %esp popfl popl %esp ret 我们简单的总结一下，page_fault_handle的栈变化情况： 陷入：用户栈->内核栈->异常栈 返回：异常栈->用户栈 如果是嵌套处理，效果也是类似的，返回中都不会经过内核栈，若page fault嵌套，那么返回过程为： 异常栈帧n->异常栈帧n-1 ... ->用户栈 exercise 11 实现/kern/pgfault.c中的set_pgfault_handler函数 set_pgfault_handler可以在用户态进行调用，并且可以在用户态自由定义处理函数。 // // Set the page fault handler function. // If there isn't one yet, _pgfault_handler will be 0. // The first time we register a handler, we need to // allocate an exception stack (one page of memory with its top // at UXSTACKTOP), and tell the kernel to call the assembly-language // _pgfault_upcall routine when a page fault occurs. // void set_pgfault_handler(void (*handler)(struct UTrapframe *utf)) { int r; if (_pgfault_handler == 0) { // First time through! // LAB 4: Your code here. envid_t envid = sys_getenvid(); if(sys_page_alloc(envid, (void *)(UXSTACKTOP-PGSIZE), PTE_U | PTE_W | PTE_P) env_id, _pgfault_upcall) Implementing Copy-on-Write Fork 通过上面的实现，目前我们已经在用户态完成了copy-on-write必要的函数功能。 在fork()的实现中，也会在child process复制parent process整个地址空间。和dumbfork()不同的是，fork()只有当某个进程尝试对页进行写操作时，才会创建新的页，使其能够进行写操作。 整个fork()流程基本如下： parent process通过set_pgfault_handler()建立pgfault()。 parent process 使用sys_exofork()去创建一个新的child process。 在parent process中UTOP以下的内存部分，若权限是可写(writable)或者是copy-on-write权限，那么就使用duppage。修改的步骤为：①将这些页映射到child process并且权限设置为copy-on-write。②重新将这些页权限改为copy-on-write（注意先设置child process再设置parent process是非常重要的，设想一个调换顺序的反例：------------）。duppage权限使用PTEs|PTE_COW来区分那些真正只读的页。 需要给child process分配一个新的异常处理栈，因为我们要对中断一场进行处理，因此不能对其使用COW。 parent process为child process设置user page fault handler entrypoint。 将一开始创建的新child process ENV_NOT_RUNNABLE设置为ENV_RUNNABLE 每当其中的一个进程对这些COW页进行写操作时，就会发生page fault。下面讲讲user page fault handler的控制流： 发生中断陷入内核，trap()会设置UTrapframe()并且陷入异常处理栈，从而能够调用pgfault() handler。 pgfault()会检测这些页是否可写，或者时权限设置为COW的页，如果不是上面的两种权限，那么报错。 pgfault()在首先在PFTEMP分配映射一个物理页，然后将之前的页内容复制到这个新分配的物理页中，重新对映射虚拟地址，从而完成新页面的设置。 exercise 12 实现/lib/fork.c中的fork，duppage和pgfault函数。 fork函数的实现：高层的COW调用 envid_t fork(void) { // LAB 4: Your code here. set_pgfault_handler(pgfault); envid_t envid = sys_exofork(); if (envid duppage函数的实现：实现内存共享 // // Map our virtual page pn (address pn*PGSIZE) into the target envid // at the same virtual address. If the page is writable or copy-on-write, // the new mapping must be created copy-on-write, and then our mapping must be // marked copy-on-write as well. (Exercise: Why do we need to mark ours // copy-on-write again if it was already copy-on-write at the beginning of // this function?) // // Returns: 0 on success, child sys_page_map failed.\"); if(sys_page_map(0, addr, 0, addr, PTE_U | PTE_COW | PTE_P) pgfault函数的实现：一旦发生冲突，就开始物理页的复制分离。 static void pgfault(struct UTrapframe *utf) { void *addr = (void *) utf->utf_fault_va; //cprintf(\"fault va = %x\\n\", utf->utf_fault_va); uint32_t err = utf->utf_err; int r; // Check that the faulting access was (1) a write, and (2) to a // copy-on-write page. If not, panic. // Hint: // Use the read-only page table mappings at uvpt // (see ). // LAB 4: Your code here. // Allocate a new page, map it at a temporary location (PFTEMP), // copy the data from the old page to the new page, then move the new // page to the old page's address. // Hint: // You should make three system calls. // LAB 4: Your code here. if(!(err & FEC_WR) || !(uvpt[PGNUM(addr)] & PTE_COW)) panic(\"pgfault: invalid UTrapFrame\"); envid_t envid = sys_getenvid(); addr = ROUNDDOWN(addr, PGSIZE); // if(envid == 4097){ // //cprintf(\"%x\\n\", addr); // int val = sys_page_map(4097, addr, 4097, addr, PTE_U | PTE_W | PTE_P); // cprintf(\"return erro %d\\n\", val); // return ; // } //cprintf(\"now allocating page: envid %d \\n\", envid); if(sys_page_alloc(envid, PFTEMP, PTE_U | PTE_W | PTE_P) _pgfault_upcall是一个通用的函数调用过程，使得程序能够从用户态陷入到异常处理栈部分。 //陷入的过程中进行调用 tf->tf_eip = (uint32_t)curenv->env_pgfault_upcall; 而_pgfault_handler部分是用户能够自由定义的page fault处理函数，在已经陷入到异常处理栈进行调用。 _pgfault_upcall: // Call the C page fault handler. pushl %esp // function argument: pointer to UTF movl _pgfault_handler, %eax call *%eax //这里进行调用 addl $4, %esp // pop function argument 有个问题：这样不会造成页面的浪费吗？因为一开始该页设置为只读，每一个尝试修改后，都会复制新页。并没有区分child process还是parent process。(需要进行实锤) 为了验证上面的想法，我们在构造函数/user/forktwice.c，并且在pgfault()函数和sys_page_map()(硬编码代码很丑)中进行相应的修改，由于该程序是parent先发生页错误中断，然后child process放生页错误中断。我们在第二次发生中断后直接将该页面改为可写。发现程序是能正确的运行的，发现确实有多余的页分配。 最后想想，发现这样提升效率并不对。设想在parent process中进行10次fork()并不知道谁最后一个获得该页面的权限，因此是一个空间换时间的操作。 Part C: Preemptive Multitasking and Inter-Process communication (IPC) Clock Interrupts and Preemption 目前JOS系统中，需要进程主动交出运行的权限，才能切换进程。如果用户进程使用while死循环，那么其他的进程和内核将永运都不能执行，这将是致命的。 为了能够让kernel主动的夺回使用权，我们需要使得JOS支持外部硬件的时钟中断。 Interrupt discipline 外界中断（IRQs）有16个，在IDT表中的位置为IRQ_OFFSET 到 IRQ_OFFSET+15. 在JOS的设置中IRQ_OFFSET=32。这样设置，是为了不让和processor exceptions重叠。 在JOS中，我们还做了简化：在用户态可以激发硬件中断，但是在内核态，我们会关闭硬件中断。 外界中断主要是通过eflags和FL_IF进行设置的，我们发现在env_alloc初始的时候，确实新增了这样的代码，使得能够在用户态产生硬件中断： // Enable interrupts while in user mode. // LAB 4: Your code here. e->env_tf.tf_eflags |= FL_IF; exercise 13 修改kern/trapentry.S和kern/trap.c，增加硬件中断的入口。 在sched_halt增加sti，取消在内核态的中断。 TRAPHANDLER_NOEC(handler_timer, IRQ_OFFSET + IRQ_TIMER) TRAPHANDLER_NOEC(handler_kbd, IRQ_OFFSET + IRQ_KBD) TRAPHANDLER_NOEC(handler_serial, IRQ_OFFSET + IRQ_SERIAL) TRAPHANDLER_NOEC(handler_spurious, IRQ_OFFSET + IRQ_SPURIOUS) TRAPHANDLER_NOEC(handler_ide, IRQ_OFFSET + IRQ_IDE) TRAPHANDLER_NOEC(handler_error, IRQ_OFFSET + IRQ_ERROR) SETGATE(idt[IRQ_OFFSET + IRQ_TIMER], 0, GD_KT, handler_timer, 0); SETGATE(idt[IRQ_OFFSET + IRQ_KBD], 0, GD_KT, handler_kbd, 0); SETGATE(idt[IRQ_OFFSET + IRQ_SERIAL], 0, GD_KT, handler_serial, 0); SETGATE(idt[IRQ_OFFSET + IRQ_SPURIOUS], 0, GD_KT, handler_spurious, 0); SETGATE(idt[IRQ_OFFSET + IRQ_IDE], 0, GD_KT, handler_ide, 0); SETGATE(idt[IRQ_OFFSET + IRQ_ERROR], 0, GD_KT, handler_error, 0); 最后，在kern/env.c 的env_alloc() 中加入e->env_tf.tf_eflags |= FL_IF; 在用户态能进行用户中断，以使得中断发生时内核能够正确处理，并取消kern/sched.c 中sti 的注释。 Handling Clock Interrupts 目前lapic_init和pic_init初始化了硬件中断的条件，使得硬件能够产生硬件中断并且能够被系统接收到。下面我们需要实现处理这些中断。 exercise 14 修改内核的trap_dispatch 函数，让它调用sched_yield() 来在时钟中断发生时查找并运行一个不同的环境。 要处理时钟中断，只需要在trap_dispatch 中添加如下代码即可。注意需要使用lapic_eoi() 来接受中断。 // Handle clock interrupts. Don't forget to acknowledge the // interrupt using lapic_eoi() before calling the scheduler! // LAB 4: Your code here. if (tf->tf_trapno == IRQ_OFFSET + IRQ_TIMER) { lapic_eoi(); sched_yield(); } Inter-Process communication (IPC) 之前所有做的工作，使得进程像是单独使用机器资源的假象。 操作系统提供的另外一个服务就是程序间的通信。这是一个强有力的能力，比如Unix pipe就是一个很典型的例子。 IPC in JOS 能够传递一个32位的数字和一页的映射。使得通信的进程共享一些页内容。 Sending and Receiving Messages 为了能够收到信息，接受environment会将自己的状态设置为ENV_NOT_RUNNALBEL并且主动使用sys_yield()主动交出CPU使用权限。 为了能够主动的发送一个值，进程会使用sys_ipc_try_send去发送，包含的参数是目标进程号。通过判断目标进程状态，看看是否处于接受信息的状态：env->env_ipc_recving。 若对方成功接收了或者对方压根没有接受，那么直接返回，否则其他情况发送进程处于忙轮询状态(while(1))。 sys_ipc_recv被ipc_recv调用，ipc_recv供用户的调用。 同理sys_ipc_send被ipc_recv调用。 Transferring Pages sys_ipc_try_send传递一个合理的需要传输的页虚拟源地址，内核首先找到物理页，然后将接受environment目标接受的区域(dstva默认初始化为UTOP，或者调用的时候指定)进行映射。 从而达到共享页的目的。 Implementing IPC exercise 15 实现在kern/syscall.c中的sys_ipc_recv和sys_ipc_try_send。注意将调用envid2env 中的checkperm flag设置为0，使得任何environment都能发送消息。 之后实现在lib/ipc.c中的ipc_recv和ipc_send。 首先实现sys_ipc_recv。需要注意的是，如果参数指示的虚拟地址大于UTOP 时不能报错退出，而需要忽略，因为这只说明接受者只需要接收值而不需要共享页面。实现的代码如下： static int sys_ipc_recv(void *dstva) { // LAB 4: Your code here. if((uint32_t)dstva env_ipc_recving = true; curenv->env_ipc_dstva = dstva; curenv->env_status = ENV_NOT_RUNNABLE; sys_yield(); return 0; //panic(\"sys_ipc_recv not implemented\"); } 接下来实现sys_ipc_try_send，需求与sys_page_map 类似，但是不能直接调用sys_page_map，因为sys_page_map 会检查env 的权限。实现的sys_ipc_try_send 如下： static int sys_ipc_try_send(envid_t envid, uint32_t value, void *srcva, unsigned perm) { // LAB 4: Your code here. struct Env *env; if(envid2env(envid, &env, 0) env_ipc_recving) return -E_IPC_NOT_RECV; pte_t *pte; int res; struct PageInfo *page = page_lookup(curenv->env_pgdir, srcva, &pte); if((uint32_t)srcva env_pgdir, page, env->env_ipc_dstva, perm)) env_ipc_recving = 0; env->env_ipc_from = curenv->env_id; env->env_ipc_perm = perm; env->env_ipc_value = value; env->env_status = ENV_RUNNABLE; return 0; //panic(\"sys_ipc_try_send not implemented\"); } 接下来实现对于上面两个系统调用的封装。首先是ipc_recv，值得注意的是当不需要共享页面时将虚拟地址设置为大于或等于UTOP 的值。ipc_recv 实现的代码如下： int32_t ipc_recv(envid_t *from_env_store, void *pg, int *perm_store) { // LAB 4: Your code here. if(!pg) pg = (void *)UTOP; int res; if((res = sys_ipc_recv(pg)) env_ipc_from; if(perm_store) *perm_store = thisenv->env_ipc_perm; return thisenv->env_ipc_value; return 0; //panic(\"ipc_recv not implemented\"); } 然后是ipc_send，同样，当pg 为NULL 时设置虚拟地址为大于或等于UTOP 的值。ipc_send实现如下： void ipc_send(envid_t to_env, uint32_t val, void *pg, int perm) { // LAB 4: Your code here. if(!pg) pg = (void *)UTOP; int res; while(1){ res = sys_ipc_try_send(to_env, val, pg, perm); if(!res) return; if(res != -E_IPC_NOT_RECV) panic(\"ipc_send: not receiving\"); sys_yield(); } //panic(\"ipc_send not implemented\"); } 最后加入系统调用： case SYS_ipc_try_send: return sys_ipc_try_send(a1, a2, (void *)a3, a4); case SYS_ipc_recv: return sys_ipc_recv((void *)a1); 番外：硬件中断是怎么进行的 我们可以看到，init_i386中有一个调用: pic_init(); 但是整个实验的篇幅很少涉及到这部分的代码，主要这一部分是对可编程中断硬件8259进行的初始化。 这里面还是大有学问的，如果对这部分比较感兴趣的同学，其实可以看看自己大学本科学习的《微机原理》里面的资料，一定会有新的发现和认识。这里就不具体进行细致的程序分析了。 "},"lab5-File system, Spawn and Shell.html":{"url":"lab5-File system, Spawn and Shell.html","title":"Lab5: File system, Spawn and Shell","keywords":"","body":"Lab 5: File system, Spawn and Shell 系统中的spawn功能（fork+exec，主要是exec需要加载硬盘中的二进制文件）和console功能（调用相关的功能时要么是遍历文件系统，要么也是加载相应的二进制文件。重定向、管道等特性也是文件系统的特性）都需要文件系统的支持。 File system preliminaries JOS的文件系统基本特征有文件的增删读写，支持文件的结构化。 JOS文件系统仅有单用户，因此不支持多用户的权限管理。同时不支持硬链接，symbolic links, 时间戳和特殊设备的读取。 On-Disk File System Structure 大多数UNIX文件系统将硬盘划分成两种区域： inode region data regions 每一个文件都会分配一个inode。inode会保存一个文件的重要的元数据，并且指向自己的数据块。 目录含有文件夹的名字和指向inode的指针。 如果一个文件的inode被多个文件目录包含，那么可以说这是文件硬链接。 JOS系统不会支持硬链接，并且JOS不支持inode的管理。JOS仅仅通过简单的保存一个文件或者目录的元数据，从而达到索引的目的（后面有图进行详细的说明） JOS允许用户态的程序读取目录的元数据，因此实现ls功能将是轻而易举的事情。这样做的缺点是应用程序文件格式需要依赖JOS目录元数据的格式，使得兼容性变得特别的差。（相当于是固定的简陋格式很难允许其他格式的程序进行运行，而inode有更好调节的灵活性） Sectors and Blocks 硬盘的基本单位是sector = 512Bytes， JOS读取的基本单位是block = 4096Bytes。 Superblocks superblock保存了文件系统总共有多少的block和根目录位置，索引其他文件夹或者文件的指针。 一般保存在block 1。之所以没保存在block 0是预留位置给boot loader和partition table。 许多系统中保存着多个superblock，因此当其中的一个块损坏后，其他的仍然能够保持整个文件系统正常的运转。 File Meta-data JOS将文件的元数据通过mmap进行分配： if ((diskmap = mmap(NULL, nblocks * BLKSIZE, PROT_READ|PROT_WRITE, MAP_SHARED, diskfd, 0)) == MAP_FAILED) panic(\"mmap %s: %s\", name, strerror(errno)); 只用记住分配了一块内存给了File 结构体元数据，使得之后能够进行指针的操作。 JOS有10个直接访问块和1个间接访问块，总共能够管理的数据大小为(10+1024)*4KB 约等于4MB。 如果想要支持更大文件的管理，则需要2个间接或者3次间接寻址的方式（xv6中就支持这种操作）。 其中管理文件数据的元数据File是通过mmap进行分配的。 Directories versus Regular Files 文件夹和普通的文件通过元数据type进行区分，JOS都是通过File指针保存到元数据中，从而进行结构的索引的。 若想找到某个文件或者文件夹，那么就去遍历10个direct block和1024个indirect block。通过比对路径的类型和文件名，从而确定是否找到并返回。其中每一个block中保存的都是struct File，如下图所示： 若文件的类型是文件，而不是文件夹，那么每一个索引的数据项10+1024=1034项中保存的都是硬盘的块号。(可以在file_block_walk和file_get_block中的函数逻辑知道) The File System 这一部分的实验并不需要我们去实现一个文件系统，但是我们需要对实现提供的一些文件的操作函数比较的熟悉。本实验新增代码约有5k行。 Disk Access JOS使用 IDE disk driver去访问硬盘数据。我们需要修改相应的文件服务进程的权限，从而使得只有该进程能够获取硬盘的数据。 通过polling和programmed I/O，我们就可以在用户态实现磁盘数据的读取。当然我们可以基于中断来实现该功能，但是这相对而言比较的难。 x86通过%eflags中的IOPL位来判断保护模式下的代码能否使用特殊的设备I/O指令如IN和out。 在JOS中，我们希望仅有file system environment（文件服务系统）有上面设备I/O的权限，其他的进程统统都没有该权限。 exercise 1 i386_init中，通过ENV_TYPE_FS判断是否是文件服务系统进程，之后给操作文件的特权。 首先我们在创建进程的时候，带有进程的种类： ENV_CREATE(fs_fs, ENV_TYPE_FS); 由于文件服务系统的特殊性，要在初始化eflags寄存器的时候进行判断，代码如下： if (type == ENV_TYPE_FS) { newenv->env_tf.tf_eflags |= FL_IOPL_MASK; } question 1 怎么保证进程切换，这样的 device I/O priviledge能够保持？ 因为该权限保存在进程的帧中，当陷入其他进程时，都会弹出相应进程当时寄存器的值，而不会错误的使用文件服务系统特有的寄存器值。如何保存栈帧和弹出栈帧，我们在之前的实验中就已经详细的阐述了。 注意本实验fs.img我们设置的是disk 1, boot loader设置的disk 0。 The Block Cache 通过系统的虚拟内存管理，我们实现一个简单的文件的buffer cache。 我们的文件系统会映射到[DISKMAP, DIAKMAP+DISKMAX] = [0X10000000, 0xD0000000] = 3G大小。 并且这一部分会映射到文件服务系统的页目录和页表中。但是这样的作法在目前的操作系统中并不正确，因为磁盘的大小动辄上T。 如果将磁盘的整个内容移动到内存中，那么代价将是非常大的。因此，我们借鉴lab4中的思想，先做映射，当发生缺页中断了，那么再进行相应的映射。 exercise 2 实现fs/bc.c中的bc_pgfault和flush_block函数。 实现bc_pgfault时注意下面的事项： addr可能不是和一个block对齐的 ide_read操作的单位是sectors(512Bytes)，而不是block(4096Bytes)。 如果某页没有被映射或者不为脏，那么flush_block不干任何事情。 其中脏位(PTE_D)是由硬件进行设置的，并且保存在uvpt中。flush_block使用sys_page_map来清除PTE_D位。 fs_init中有着如何使用block cache的例子。 当block cache初始化完后（实际上仅仅是设置了一个bc_pgfault的处理函数），当我们访问相关的内容时，如果内容不在内存中，那么就会触发缺页中断。但是整个过程就像是内容就在内存中一样。 static void bc_pgfault(struct UTrapframe *utf) { void *addr = (void *) utf->utf_fault_va; uint32_t blockno = ((uint32_t)addr - DISKMAP) / BLKSIZE; int r; // Check that the fault was within the block cache region if (addr = (void*)(DISKMAP + DISKSIZE)) panic(\"page fault in FS: eip %08x, va %08x, err %04x\", utf->utf_eip, addr, utf->utf_err); // Sanity check the block number. if (super && blockno >= super->s_nblocks) panic(\"reading non-existent block %08x\\n\", blockno); // Allocate a page in the disk map region, read the contents // of the block from the disk into that page. // Hint: first round addr to page boundary. fs/ide.c has code to read // the disk. // // LAB 5: you code here: // envid 传入 0表示当前进程。 addr =(void *) ROUNDDOWN(addr, PGSIZE); if ( (r = sys_page_alloc(0, addr, PTE_P|PTE_W|PTE_U)) // Flush the contents of the block containing VA out to disk if // necessary, then clear the PTE_D bit using sys_page_map. // If the block is not in the block cache or is not dirty, does // nothing. // Hint: Use va_is_mapped, va_is_dirty, and ide_write. // Hint: Use the PTE_SYSCALL constant when calling sys_page_map. // Hint: Don't forget to round addr down. void flush_block(void *addr) { uint32_t blockno = ((uint32_t)addr - DISKMAP) / BLKSIZE; if (addr = (void*)(DISKMAP + DISKSIZE)) panic(\"flush_block of bad va %08x\", addr); // LAB 5: Your code here. addr = (void *)ROUNDDOWN(addr, PGSIZE); if (va_is_mapped(addr) && va_is_dirty(addr)) { ide_write(blockno*BLKSECTS, addr , BLKSECTS); int r; // Clear the dirty bit for the disk block page since we just read the // block from disk //下面的语句就是为了清空脏位 if ((r = sys_page_map(0, addr, 0, addr, uvpt[PGNUM(addr)] & PTE_SYSCALL)) The Block Bitmap 我们可以使用bitmap来记录硬盘块的使用情况。每一位都能记录一个块是否被使用。bitmap大小为4096Bytes=40968bits，那么一共管理$4096 8 * 4KB = 128MB$反正目前是够用的。 exercise 3 使用free_block为模板，来实现fs/fs.c中的alloc_block函数。 我们需要找到第一个没有使用的硬盘块，将其标记被使用，并且返回序号 注意当我们改变了bitmap时，应该将其立刻写回硬盘中，以此来保证文件系统的一致性。 // Search the bitmap for a free block and allocate it. When you // allocate a block, immediately flush the changed bitmap block // to disk. // // Return block number allocated on success, // -E_NO_DISK if we are out of blocks. // // Hint: use free_block as an example for manipulating the bitmap. uint32_t *bitmap; // bitmap blocks mapped in memory int alloc_block(void) { // The bitmap consists of one or more blocks. A single bitmap block // contains the in-use bits for BLKBITSIZE blocks. There are // super->s_nblocks blocks in the disk altogether. // LAB 5: Your code here. size_t i; //bitmap 1表示是空闲的，0表示的是占用的 for(i=1; i s_nblocks; i++) { if (block_is_free(i)) { bitmap[i/32] &= ~(1 File Operations 熟读fs/fs.c中的代码，有较为熟悉的了解。 exercise 4 实现file_block_walk和file_get_block。 file_block_walk就是不断递归，寻找绝对路径中的文件夹和文件，而file_get_block就是获得文件块中具体的块号。 下面是寻找某个文件过程的示意图： // Find the disk block number slot for the 'filebno'th block in file 'f'. // Set '*ppdiskbno' to point to that slot. // The slot will be one of the f->f_direct[] entries, // or an entry in the indirect block. // When 'alloc' is set, this function will allocate an indirect block // if necessary. // // Returns: // 0 on success (but note that *ppdiskbno might equal 0). // -E_NOT_FOUND if the function needed to allocate an indirect block, but // alloc was 0. // -E_NO_DISK if there's no space on the disk for an indirect block. // -E_INVAL if filebno is out of range (it's >= NDIRECT + NINDIRECT). // // Analogy: This is like pgdir_walk for files. // Hint: Don't forget to clear any block you allocate. static int file_block_walk(struct File *f, uint32_t filebno, uint32_t **ppdiskbno, bool alloc) { int r; if (filebno >= NDIRECT + NINDIRECT) return -E_INVAL; if (filebno f_direct + filebno; return 0; } if (!alloc && !f->f_indirect) return -E_NOT_FOUND; //间接寻找 if (!f->f_indirect) { if ((r = alloc_block()) f_indirect = r; memset(diskaddr(r), 0, BLKSIZE); flush_block(diskaddr(r)); } //这里保存的是硬盘块号，有点不理解 if (ppdiskbno) *ppdiskbno = (uint32_t*)(diskaddr(f->f_indirect)) + filebno - NDIRECT; return 0; } // Set *blk to the address in memory where the filebno'th // block of file 'f' would be mapped. // // Returns 0 on success, file_block_walk and file_get_block是file system的马力。file_read和file_write正是基于上面的两个函数实现调用的。 The file system interface JOS将文件功能单独的独立出来一个文件服务进程，并且其他的进程没有权限调用相关的文件操作函数（由于eflags寄存器的限制）。因此，如果其他进程想要对文件进行操作，那么必须和文件系统进程进行通信，通信的结构如下： Regular env FS env +---------------+ +---------------+ | read | | file_read | | (lib/fd.c) | | (fs/fs.c) | ...|.......|.......|...|.......^.......|............... | v | | | | RPC mechanism | devfile_read | | serve_read | | (lib/file.c) | | (fs/serv.c) | | | | | ^ | | v | | | | | fsipc | | serve | | (lib/file.c) | | (fs/serv.c) | | | | | ^ | | v | | | | | ipc_send | | ipc_recv | | | | | ^ | +-------|-------+ +-------|-------+ | | +-------------------+ exercise 5 实现/fs/serv.c中的serve_read函数 // Read at most ipc->read.req_n bytes from the current seek position // in ipc->read.req_fileid. Return the bytes read from the file to // the caller in ipc->readRet, then update the seek position. Returns // the number of bytes successfully read, or read; struct Fsret_read *ret = &ipc->readRet; if (debug) cprintf(\"serve_read %08x %08x %08x\\n\", envid, req->req_fileid, req->req_n); // Lab 5: Your code here: int r; struct OpenFile *of; if ( (r = openfile_lookup(envid, req->req_fileid, &of) )o_file, ret->ret_buf, req->req_n, of->o_fd->fd_offset))o_fd->fd_offset += r; return r; } exercise 6 实现fs/serv.c中的serve_write函数，实现lib/file.c中的devfile_write函数。 // Write req->req_n bytes from req->req_buf to req_fileid, starting at // the current seek position, and update the seek position // accordingly. Extend the file if necessary. Returns the number of // bytes written, or req_fileid, req->req_n); // LAB 5: Your code here. int r; struct OpenFile *of; int reqn; if ( (r = openfile_lookup(envid, req->req_fileid, &of)) req_n > PGSIZE? PGSIZE:req->req_n; if ( (r = file_write(of->o_file, req->req_buf, reqn, of->o_fd->fd_offset)) o_fd->fd_offset += r; return r; //panic(\"serve_write not implemented\"); } devfile_write函数： static ssize_t devfile_write(struct Fd *fd, const void *buf, size_t n) { // Make an FSREQ_WRITE request to the file system server. Be // careful: fsipcbuf.write.req_buf is only so large, but // remember that write is always allowed to write *fewer* // bytes than requested. // LAB 5: Your code here int r; if (n > sizeof(fsipcbuf.write.req_buf)) n = sizeof(fsipcbuf.write.req_buf); fsipcbuf.write.req_fileid = fd->fd_file.id; fsipcbuf.write.req_n = n; memmove(fsipcbuf.write.req_buf, buf, n); if ((r = fsipc(FSREQ_WRITE, NULL)) Spawning Processes spawn = fork + exec exercise 7 实现kern/syscall.c中的sys_env_set_trapframe。 // Set envid's trap frame to 'tf'. // tf is modified to make sure that user environments always run at code // protection level 3 (CPL 3), interrupts enabled, and IOPL of 0. // // Returns 0 on success, env_tf = *tf; env->env_tf.tf_cs |= 0x3; env->env_tf.tf_eflags &= (~FL_IOPL_MASK); env->env_tf.tf_eflags |= FL_IF; return 0; //panic(\"sys_env_set_trapframe not implemented\"); } Sharing library state across fork and spawn 分享的部分其实就是为了能够进行内存的共享，从而能够进行内容的互换与通信。 之前所有做的父进程与子进程之间内存都是共享读的，一旦某一个进程改变了其中的内容，那么父进程和子进程分享读的内容就分道扬镳了。 为了能够实现页的共享，JOS做了下面的设计与规定： 新增符号位PTE_SHARE，使得有该标记的页能够被共享。 exercise 8 更改lib/fork.c中duppage函数的实现，如果页有PTE_SHARE标记位，那么直接进行copy。 同理，实现lib/spawn.c中的copy_shared_pages函数，它会将目前进程中所有的页表遍历一遍，然后copy那么标有PTE_SHARE的页。 static int duppage(envid_t envid, unsigned pn) { int r; // LAB 4: Your code here. int ret; void *addr = (void *)(pn * PGSIZE); // 新增的分支判断 if (uvpt[pn] & PTE_SHARE) { if((ret = sys_page_map(0, addr, envid, addr, uvpt[pn] & PTE_SYSCALL)) child sys_page_map failed.\"); if(sys_page_map(0, addr, 0, addr, PTE_U | PTE_COW | PTE_P) // Copy the mappings for shared pages into the child address space. static int copy_shared_pages(envid_t child) { // LAB 5: Your code here. size_t pn; int r; struct Env *e; for (pn = PGNUM(UTEXT); pn > 10] & PTE_P) && (uvpt[pn] & PTE_P) ) { if (uvpt[pn] & PTE_SHARE) { if ( (r = sys_page_map(thisenv->env_id, (void *)(pn*PGSIZE), child, (void *)(pn*PGSIZE), uvpt[pn] & PTE_SYSCALL )) pipe的本质 在JOS中，pipe本质上就是一段父进程和子进程共享的一段内存。很多的示意图会将管道画成一个队列的形式，其实并不正确，下面通过pipe的初始化和使用，对pipe的原理进行说明。 一般父进程和子进程进行通信，假设子进程读，父进程写，那么一般的套路为： char *msg = \"Now is the time for all good men to come to the aid of their party.\"; char buf[100]; int p[2]; pipe(p); int pid = fork(); if(pid == 0){ //子进程程序 close(p[1]); i = readn(p[0], buf, sizeof buf-1); exit(); } else{ //父进程程序 close(p[0]); i = write(p[1], msg, strlen(msg)); close(p[1]); } wait(pid); pipe读写的过程：写和读操作都是在同一个内存块的。 if ((r = sys_page_map(0, va, 0, fd2data(fd1), PTE_P|PTE_W|PTE_U|PTE_SHARE)) 该语句表示的就是两个fd指向的是同一个虚拟的页面。 close(p[i])实际上是减少该内存页的引用次数。 读是在一个轮询的状态，因此当一个进程完成了写操作，那么就应该立刻close(p[1])，从而能够减少该共享页的引用次数。而读进程一旦知道了所有的写操作都完成后，那么就会结束这个读的操作。如果不及时的关闭写，那么读进程就会进入死循环（写进程是正常结束的）。 同样的，如果写进程写满了pipe buffer，并且检测到没有读管道，那么会立刻停止写的操作。 上图是整个文件系统大概的工作流程，fd实际上是dev(即硬盘)、管道和console的抽象。在lab6中也是socket的抽象。 我们特意简化其中的一些要素。 下面是整个pipe工作的原理图： fd0(读操作)，fd1(写操作)都指向了同样的一块虚拟地址空间，这个空间就是pipe。其中p_rpos是读指针，p_wpos是写指针，它们共同的数据区域为p_buf。 当某个进程写完了，应该及时的关闭管道，这样才能够及时的通知读进程，告诉它读完就可以结束了。 ret = pageref(fd) == pageref(p); 这个语句中，当写进程在写的时候，pageref(fd) ，一旦close(fd[i])，相关的pageref(p)就会减少。注意p只有一个，而fd有两个fd0和fd1。 知道了上面的原理，我们修改上面的代码： if(pid == 0){ //子进程程序 close(p[1]); i = readn(p[0], buf, sizeof buf-1); exit(); } else{ //父进程程序 //close(p[0]); //仅仅把这个注释了 i = write(p[1], msg, strlen(msg)); close(p[1]); } wait(pid); 这样也是对的。 一开始pipe()+fork()，使得pageref(p)=4, pageref(fd0)=2, pageref(fd1)=2。 我们只要保证写进程写完后pageref(fd1) = 0就能保证程序的正确的运行，而不会使得读进程陷入死循环。 The keyboard interface 这一部分的实现和在monitor中的输入不太一样，这一部分是在用户态的程序进行输入的接受。 exercise 9 In your kern/trap.c, call kbd_intr to handle trap IRQ_OFFSET+IRQ_KBD and serial_intr to handle trap IRQ_OFFSET+IRQ_SERIAL. // Handle keyboard and serial interrupts. // LAB 5: Your code here. if (tf->tf_trapno == IRQ_OFFSET + IRQ_KBD) { kbd_intr(); return; } if (tf->tf_trapno == IRQ_OFFSET + IRQ_SERIAL) { serial_intr(); return; } The Shell 运行make run-icode或者运行make run-icode-nox，使得能够通过键盘输入运行简单的应用程序。 exercise 10 进行相关的实现，使得该程序支持输入的重定向，支持sh 操作。 解析读入后面的文件，然后将该文件作为运行程序的输入即可。 case ' 最后的结果： "},"lab6-Network Driver.html":{"url":"lab6-Network Driver.html","title":"Lab6: Network Driver","keywords":"","body":"Lab 6: Network Driver 这一部分感觉最重要的就是对硬件的驱动过程有了一个更加深刻的认识，本实验相当于是去实现一个网卡的驱动，并且在这个驱动上，开发相应的应用程序（echosrv和httpd）。 首先是pci_init()，这一部分主要就是扫描PIC总线上的一些设备，与系统内核中带有的驱动程序进行匹配。 通过PCI的扫描，我们发现qemu提供下面几种设别： 本实验主要是实现网卡E1000的驱动，通过查阅相关intel网卡的手册，发现Vendor ID和Device ID如下所示： PCI主要就是通过上面的两个变量进行匹配的，一旦匹配，说明系统中存在该设备的驱动，就能够利用该设备了。 其他的设备由于没有在系统中完成相关的驱动程序，所以也没有设置Vendor ID和Device ID。 有了设备后，为了能够方便对设备进行操作，我们使用lab1提到的MMIO（VGA）和lab4中对多核CPU lapic的初始化。MMIO主要的作用就是像是操作内存一样的去操作硬件，实际上访问内存地址的时候，访问的是硬件的寄存器地址。 但是MMIO地址空间和大小并不是系统进行决定的，而是和设备进行协商进行确定（感觉是要是知道MMIO空间的大小），因此主要工作就是初始化这个结构体： struct pci_func { struct pci_bus *bus; // Primary bus for bridges uint32_t dev; uint32_t func; uint32_t dev_id; uint32_t dev_class; uint32_t reg_base[6];//最多6个MMIO区域 uint32_t reg_size[6];//每个MMIO区域的大小 uint8_t irq_line;//中断的地址线，上面PCI线扫描显示网卡的中断使用的是11 }; 然后系统通过这个结构体的信息，来初始化e1000信息，这些初始化的信息是直接写到硬件中的（虽然感觉像是操作内存变量）。 pci_e1000_attach(struct pci_func * pcif) { pci_func_enable(pcif); init_desc(); e1000 = mmio_map_region(pcif->reg_base[0], pcif->reg_size[0]); cprintf(\"e1000: bar0 %x size0 %x\\n\", pcif->reg_base[0], pcif->reg_size[0]); e1000[TDBAL/4] = PADDR(tx_d); e1000[TDBAH/4] = 0; e1000[TDLEN/4] = TXRING_LEN * sizeof(struct tx_desc); e1000[TDH/4] = 0; e1000[TDT/4] = 0; e1000[TCTL/4] = TCTL_EN | TCTL_PSP | (TCTL_CT & (0x10 还需要说明的就是E1000网卡使用的是DMA技术与JOS进行交互。 也就是说，往内存里读写网络数据包不需要通过CPU，实际上还是通过MMIO这块区域进行操作。本实验基本上没有涉及DMA的代码，因此了解就好了。 因此，我们就能很轻松的使用这张网卡了。 下面是一些系统调用的实现。 首先是实现系统的定时中断。JOS中设置的是每250ms来让server进程检测是否有包要接受或者发送。（Linux系统中是4ms，精度可以调的更高，但是带来的开销也是越大的。从这里就可以看到Linux并不是一个实时的操作系统，而是一个分时操作系统，涉及到时间操作的精度并不是很高）。 然后实现发包，为什么呢？因为要是没有实现发包，那么收包也没有东西可以收。 这一部分的测试在/net/testoutput.c中，可以结合着测试的样例和具体的实现原理来阅读代码。 然后实现的是input的模块。 有了上面的四个模块基础之后，之后将会在这四个模块的基础之上，实现两个应用：echosrv和httpd。 echosrv主要的作用就是收到一个连接的信息后，返回相同的信息。 服务端： make E1000_DEBUG=TX,TXERR,RX,RXERR,RXFILTER run-echosrv 客户端： make nc-7 下面说一说echosrv的建立连接的原理： 首先是向网络服务进程发送一个IPC请求，然后一直处于接受状态： int res = ipc_recv(NULL, NULL, NULL); 该等待是基于事件进行触发的，因此如果没有请求来，那么根本不会占用任何的计算资源（因为ipc_recv是将进程的状态设置为了curenv->env_status = ENV_NOT_RUNNABLE，因此每一次轮询都不会执行该进程）。 同理，httpd也是类似的作用，使用的是80端口进行服务，并且能通过url进行文件的访问。 这个实现的框架标记了实现的过程，结构如下： "},"hw-boot xv6.html":{"url":"hw-boot xv6.html","title":"boot xv6","keywords":"","body":"Homework: boot xv6 boot XV6 首先搭建相应的实现环境，具体可以参考实验环境搭建。 按照下面的指令进行clone和编译： Fetch the xv6 source: $ mkdir 6.828 $ cd 6.828 $ git clone git://github.com/mit-pdos/xv6-public.git Build xv6 on Athena: $ cd xv6-public $ make Finding and breaking at an address ​ 这一部分就是让我们找到相关的函数的地址，并且在这个地址打上断点，便于之后的调试。 ​ 这里以内核的启动入口为例，首先我们使用nm1工具找到内核的入口0x0010000c，之后在这个地址打上断点br * 0x0010000c，之后就能通过gdb的si指令进行单步的调试了。 ​ 关于这个内核的启动地址为什么是0x100000c，这个在kernel.ld固定设置，并且是一个不成文的规定。 # 一个终端： make qemu-gdb # 另外一个终端： make gdb Exercise: What is on the stack? ​ 本部分希望我们了解函数调用时栈发生了什么变化。 Q1：什么时候初始化了栈空间？ bootasm.S: movl $start, %esp # $start==0x7c00 26│ 0x7c43: mov $0x7c00,%sp 27│ 0x7c46: add %al,(%bx,%si) 28├──> 0x7c48: call 0x7d39 29│ 0x7c4b: add %al,(%bx,%si) 30│ 0x7c4d: mov $0x89668a00,%eax 31│ 0x7c53: ret $0xef66 32│ 0x7c56: mov $0xef668ae0,%eax 33│ 0x7c5c: jmp 0x7c5c 34│ 0x7c5e: xchg %eax,%eax 35│ 0x7c60: add %al,(%bx,%si) 36│ 0x7c62: add %al,(%bx,%si) 37│ 0x7c64: add %al,(%bx,%si) 38│ 0x7c66: add %al,(%bx,%si) 39│ 0x7c68: (bad) 40│ 0x7c69: incw (%bx,%si) 41│ 0x7c6b: add %al,(%bx,%si) ** 0x7c00: cli (7c00 - 7cea) ** gs 0x0 0 (gdb) b *0x7c48 Breakpoint 2 at 0x7c48 (gdb) c Continuing. The target architecture is assumed to be i386 => 0x7c48: call 0x7d3b Breakpoint 2, 0x00007c48 in ?? () (gdb) info reg eax 0x0 0 ecx 0x0 0 edx 0x80 128 ebx 0x0 0 esp 0x7c00 0x7c00 ebp 0x0 0x0 esi 0x0 0 edi 0x0 0 eip 0x7c48 0x7c48 eflags 0x6 [ PF ] cs 0x8 8 ss 0x10 16 ds 0x10 16 es 0x10 16 fs 0x0 0 gs 0x0 0 (gdb) Q2: 单步执行到bootmain，看看栈发生了什么？ call bootmain 7c48: e8 ee 00 00 00 call 7d3b 此刻esp = 0x7c00变化为esp = 0x7bfc，并且里面保存的值为0x7c4d，连保存的是call bootmain的后面一个语句的执行地址。 之后是一系列的压栈操作： 7d3b: 55 push %ebp 7d3c: 89 e5 mov %esp,%ebp 7d3e: 57 push %edi 7d3f: 56 push %esi 7d40: 53 push %ebx 7d41: 83 ec 0c sub $0xc,%esp 此时esp = 0x7be0, ebp=0x7bf8。 Q3: 第一个操作栈的指令是什么？ 7d3b: 55 push %ebp 7d3c: 89 e5 mov %esp,%ebp 可以看到是对原来的ebp进行保存，并且将原来的esp当作新的函数调用的栈底。 Q4：调用到0x10000c之前，栈寄存器发生了什么？ 我们主要理解栈里面填充的内容是哪些指令来的。 之后我们在调用kernel入口函数之前打上一个断点： 发现期间函数栈没有发生变化。 执行完call指令之后，发现还是相同的套路，esp = esp-4，并且保存的值是call 指令后面的那个地址，即 0x7db4。 红框中的值是两个函数调用的返回地址值，期间的若干值是下面的指令产生的： 7d3b: 55 push %ebp 7d3c: 89 e5 mov %esp,%ebp 7d3e: 57 push %edi 7d3f: 56 push %esi 7d40: 53 push %ebx 7d41: 83 ec 0c sub $0xc,%esp ... 7d82: ff 73 04 pushl 0x4(%ebx) 7d85: ff 73 10 pushl 0x10(%ebx) 7d88: 57 push %edi 正好是7个push的指令。 1. nm - list symbols from object files ↩ "},"hw-syscall.html":{"url":"hw-syscall.html","title":"syscall","keywords":"","body":"hw-syscall part1 part2 syscall调用的过程 ​ 在用户层面涉及文件有usys.s, user.h。我们发现在用户态调用的时候，我们的函数调用是带有参数的，但是陷入到系统调用中，函数中是不带有参数的。 举例： int date(struct rtcdate *); user.h int sys_date(void); sysproc.c ​ 因此，我很好奇syscall是怎么调用的。 ​ 最后仔细的阅读源代码，发现系统调用中虽然不带有参数，但是由于函数调用的结构化，我们在系统调用的时候可以从内存中主动去读取相关的变量值，从而能够顺利的执行相关的函数功能。不带有参数是为了系统调用时结构的统一性。 "},"hw-alarmtest.html":{"url":"hw-alarmtest.html","title":"alarmtest","keywords":"","body":"alarmtest 指针解读 *(uint *)(tf->esp) = tf->eip; 这个语句比较的复杂，其中tf->esp本来是uint类型的数值，(uint )(tf->esp)其实还是存着原来的数值，即(uint )(tf->esp) == tf->esp，但是再加上一个*表示取这个值对应的地址的内容。最后的结果就是tf->esp所指向的地址里面的内容填充tf->eip。 "}}